{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting a decision stump\n",
    "\n",
    "The goal of this notebook is to implement your own boosting module.\n",
    "\n",
    "**Brace yourselves**! This is going to be a fun and challenging assignment.\n",
    "\n",
    "\n",
    "* Use SFrames to do some feature engineering.\n",
    "* Modify the decision trees to incorporate weights.\n",
    "* Implement Adaboost ensembling.\n",
    "* Use your implementation of Adaboost to train a boosted decision stump ensemble.\n",
    "* Evaluate the effect of boosting (adding more decision stumps) on performance of the model.\n",
    "* Explore the robustness of Adaboost to overfitting.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire up GraphLab Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of GraphLab Create **(1.8.3 or newer)**. Upgrade by\n",
    "```\n",
    "   pip install graphlab-create --upgrade\n",
    "```\n",
    "See [this page](https://dato.com/download/) for detailed instructions on upgrading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same [LendingClub](https://www.lendingclub.com/) dataset as in the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\HONGWEI\\AppData\\Local\\Temp\\graphlab_server_1487793390.log.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to hongwei.luan@uconn.edu and will expire on January 14, 2018.\n"
     ]
    }
   ],
   "source": [
    "loans = graphlab.SFrame('lending-club-data.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the target and the feature columns\n",
    "\n",
    "We will now repeat some of the feature processing steps that we saw in the previous assignment:\n",
    "\n",
    "First, we re-assign the target to have +1 as a safe (good) loan, and -1 as a risky (bad) loan.\n",
    "\n",
    "Next, we select four categorical features: \n",
    "1. grade of the loan \n",
    "2. the length of the loan term\n",
    "3. the home ownership status: own, mortgage, rent\n",
    "4. number of years of employment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Less than 8 successfully started. Using only 7 workers.</pre>"
      ],
      "text/plain": [
       "Less than 8 successfully started. Using only 7 workers."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>All operations will proceed as normal, but lambda operations will not be able to use all available cores.</pre>"
      ],
      "text/plain": [
       "All operations will proceed as normal, but lambda operations will not be able to use all available cores."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>(The location of the log file is printed at the start of the GraphLab server).</pre>"
      ],
      "text/plain": [
       "(The location of the log file is printed at the start of the GraphLab server)."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans.remove_column('bad_loans')\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample dataset to make sure classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did in the previous assignment, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We use `seed=1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of safe loans                 : 0.502236174422\n",
      "Percentage of risky loans                : 0.497763825578\n",
      "Total number of loans in our new dataset : 46508\n"
     ]
    }
   ],
   "source": [
    "safe_loans_raw = loans[loans[target] == 1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "risky_loans = risky_loans_raw\n",
    "safe_loans = safe_loans_raw.sample(percentage, seed=1)\n",
    "loans_data = risky_loans_raw.append(safe_loans)\n",
    "\n",
    "print \"Percentage of safe loans                 :\", len(safe_loans) / float(len(loans_data))\n",
    "print \"Percentage of risky loans                :\", len(risky_loans) / float(len(loans_data))\n",
    "print \"Total number of loans in our new dataset :\", len(loans_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are many approaches for dealing with imbalanced data, including some where we modify the learning algorithm. These approaches are beyond the scope of this course, but some of them are reviewed in this [paper](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5128907&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F69%2F5173046%2F05128907.pdf%3Farnumber%3D5128907 ). For this assignment, we use the simplest possible approach, where we subsample the overly represented class to get a more balanced dataset. In general, and especially when the data is highly imbalanced, we recommend using more advanced methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will work with **binary decision trees**. Since all of our features are currently categorical features, we want to turn them into binary features using 1-hot encoding. \n",
    "\n",
    "We can do so with the following code block (see the first assignments for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans_data = risky_loans.append(safe_loans)\n",
    "for feature in features:\n",
    "    loans_data_one_hot_encoded = loans_data[feature].apply(lambda x: {x: 1})    \n",
    "    loans_data_unpacked = loans_data_one_hot_encoded.unpack(column_name_prefix=feature)\n",
    "    \n",
    "    # Change None's to 0's\n",
    "    for column in loans_data_unpacked.column_names():\n",
    "        loans_data_unpacked[column] = loans_data_unpacked[column].fillna(0)\n",
    "\n",
    "    loans_data.remove_column(feature)\n",
    "    loans_data.add_columns(loans_data_unpacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the feature columns look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade.A',\n",
       " 'grade.B',\n",
       " 'grade.C',\n",
       " 'grade.D',\n",
       " 'grade.E',\n",
       " 'grade.F',\n",
       " 'grade.G',\n",
       " 'term. 36 months',\n",
       " 'term. 60 months',\n",
       " 'home_ownership.MORTGAGE',\n",
       " 'home_ownership.OTHER',\n",
       " 'home_ownership.OWN',\n",
       " 'home_ownership.RENT',\n",
       " 'emp_length.1 year',\n",
       " 'emp_length.10+ years',\n",
       " 'emp_length.2 years',\n",
       " 'emp_length.3 years',\n",
       " 'emp_length.4 years',\n",
       " 'emp_length.5 years',\n",
       " 'emp_length.6 years',\n",
       " 'emp_length.7 years',\n",
       " 'emp_length.8 years',\n",
       " 'emp_length.9 years',\n",
       " 'emp_length.< 1 year',\n",
       " 'emp_length.n/a']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = loans_data.column_names()\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "We split the data into training and test sets with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = loans_data.random_split(0.8, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify our decision tree code from Module 5 to support weighting of individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted error definition\n",
    "\n",
    "Consider a model with $N$ data points with:\n",
    "* Predictions $\\hat{y}_1 ... \\hat{y}_n$ \n",
    "* Target $y_1 ... y_n$ \n",
    "* Data point weights $\\alpha_1 ... \\alpha_n$.\n",
    "\n",
    "Then the **weighted error** is defined by:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "where $1[y_i \\neq \\hat{y_i}]$ is an indicator function that is set to $1$ if $y_i \\neq \\hat{y_i}$.\n",
    "\n",
    "\n",
    "### Write a function to compute weight of mistakes\n",
    "\n",
    "Write a function that calculates the weight of mistakes for making the \"weighted-majority\" predictions for a dataset. The function accepts two inputs:\n",
    "* `labels_in_node`: Targets $y_1 ... y_n$ \n",
    "* `data_weights`: Data point weights $\\alpha_1 ... \\alpha_n$\n",
    "\n",
    "We are interested in computing the (total) weight of mistakes, i.e.\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}].\n",
    "$$\n",
    "This quantity is analogous to the number of mistakes, except that each mistake now carries different weight. It is related to the weighted error in the following way:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "\n",
    "The function **intermediate_node_weighted_mistakes** should first compute two weights: \n",
    " * $\\mathrm{WM}_{-1}$: weight of mistakes when all predictions are $\\hat{y}_i = -1$ i.e $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{-1}$)\n",
    " * $\\mathrm{WM}_{+1}$: weight of mistakes when all predictions are $\\hat{y}_i = +1$ i.e $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{+1}$)\n",
    " \n",
    " where $\\mathbf{-1}$ and $\\mathbf{+1}$ are vectors where all values are -1 and +1 respectively.\n",
    " \n",
    "After computing $\\mathrm{WM}_{-1}$ and $\\mathrm{WM}_{+1}$, the function **intermediate_node_weighted_mistakes** should return the lower of the two weights of mistakes, along with the class associated with that weight. We have provided a skeleton for you with `YOUR CODE HERE` to be filled in several places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_negative = total_weight_positive\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_positive = total_weight_negative\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    ### YOUR CODE HERE\n",
    "    if weighted_mistakes_all_positive <= weighted_mistakes_all_negative:\n",
    "        return weighted_mistakes_all_positive, +1\n",
    "    else:\n",
    "        return weighted_mistakes_all_negative, -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Test your **intermediate_node_weighted_mistakes** function, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = graphlab.SArray([-1, -1, 1, 1, 1])\n",
    "example_data_weights = graphlab.SArray([1., 2., .5, 1., 1.])\n",
    "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "**Quiz Question:** If we set the weights $\\mathbf{\\alpha} = 1$ for all data points, how is the weight of mistakes $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ related to the `classification error`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We continue modifying our decision tree code from the earlier assignment to incorporate weighting of individual data points. The next step is to pick the best feature to split on.\n",
    "\n",
    "The **best_splitting_feature** function is similar to the one from the earlier assignment with two minor modifications:\n",
    "  1. The function **best_splitting_feature** should now accept an extra parameter `data_weights` to take account of weights of data points.\n",
    "  2. Instead of computing the number of mistakes in the left and right side of the split, we compute the weight of mistakes for both sides, add up the two weights, and divide it by the total weight of the data.\n",
    "  \n",
    "Complete the following function. Comments starting with `DIFFERENT HERE` mark the sections where the weighted version differs from the original implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If the data is identical in each feature, this function should return None\n",
    "\n",
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    print data_weights\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        ## YOUR CODE HERE\n",
    "        left_data_weights = data_weights[data[feature] == 0]\n",
    "        right_data_weights = data_weights[data[feature] == 1]\n",
    "            \n",
    "        # DIFFERENT HERE\n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        ## YOUR CODE HERE\n",
    "        left_weighted_mistakes, left_class = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
    "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
    "        \n",
    "        # DIFFERENT HERE\n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        ## YOUR CODE HERE\n",
    "        error = (left_weighted_mistakes + right_weighted_mistakes) / (sum(left_data_weights) + sum(right_data_weights))\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Now, we have another checkpoint to make sure you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, ... ]\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = graphlab.SArray(len(train_data)* [1.5])\n",
    "if best_splitting_feature(train_data, features, target, example_data_weights) == 'term. 36 months':\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. If you get an exception in the line of \"the logical filter has different size than the array\", try upgradting your GraphLab Create installation to 1.8.3 or newer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very Optional**. Relationship between weighted error and weight of mistakes\n",
    "\n",
    "By definition, the weighted error is the weight of mistakes divided by the weight of all data points, so\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i} = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}.\n",
    "$$\n",
    "\n",
    "In the code above, we obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ from the two weights of mistakes from both sides, $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}})$ and $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})$. First, notice that the overall weight of mistakes $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ can be broken into two weights of mistakes over either side of the split:\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    "= \\sum_{\\mathrm{left}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    " + \\sum_{\\mathrm{right}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\\\\\n",
    "= \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})\n",
    "$$\n",
    "We then divide through by the total weight of all data points to obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\frac{\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. Recall from the previous assignments that each node in the decision tree is represented as a dictionary which contains the following keys:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'features_remaining' : List of features that are posible splits.\n",
    "    }\n",
    "    \n",
    "Let us start with a function that creates a leaf node given a set of target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    leaf['prediction'] = best_class ## YOUR CODE HERE\n",
    "    \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function that learns a weighted decision tree recursively and implements 3 stopping conditions:\n",
    "1. All data points in a node are from the same class.\n",
    "2. No more features to split on.\n",
    "3. Stop growing the tree when the tree depth reaches **max_depth**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print \"Stopping condition 1 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print \"Stopping condition 2 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print \"Reached maximum depth. Stopping for now.\"\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print \"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = weighted_decision_tree_create(\n",
    "        left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(\n",
    "        right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test code to check your implementation. Make sure you get **'Test passed'** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature grade.A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9122 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (101 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature grade.D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (23300 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4701 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = graphlab.SArray([1.0 for i in range(len(train_data))])\n",
    "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'\n",
    "    print 'Number of nodes found:', count_nodes(small_data_decision_tree)\n",
    "    print 'Number of nodes that should be there: 7' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a quick look at what the trained tree is like. You should get something that looks like the following\n",
    "\n",
    "```\n",
    "{'is_leaf': False,\n",
    "    'left': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade.A'\n",
    "     },\n",
    "    'prediction': None,\n",
    "    'right': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade.D'\n",
    "     },\n",
    "     'splitting_feature': 'term. 36 months'\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade.A'},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade.D'},\n",
       " 'splitting_feature': 'term. 36 months'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with a weighted decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you a function that classifies one data point. It can also return the probability if you want to play around with that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the tree\n",
    "\n",
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
    "\n",
    "Again, recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "The function called **evaluate_classification_error** takes in as input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (an SFrame)\n",
    "\n",
    "The function does not change because of adding data point weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x))\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3981042654028436"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Training a weighted decision tree\n",
    "\n",
    "To build intuition on how weighted data points affect the tree being built, consider the following:\n",
    "\n",
    "Suppose we only care about making good predictions for the **first 10 and last 10 items** in `train_data`, we assign weights:\n",
    "* 1 to the last 10 items \n",
    "* 1 to the first 10 items \n",
    "* and 0 to the rest. \n",
    "\n",
    "Let us fit a weighted decision tree with `max_depth = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]\n",
      "Split on feature home_ownership.RENT. (20514, 16710)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (20514 data points).\n",
      "[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]\n",
      "Split on feature grade.F. (19613, 901)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19613 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (901 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16710 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]\n",
      "Split on feature grade.D. (13315, 3395)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (13315 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3395 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = graphlab.SArray([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the classification error on the `subset_20`, i.e. the subset of data points whose weight is 1 (namely the first and last 10 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48124865678057166"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `small_data_decision_tree_subset_20` performs **a lot** better on `subset_20` than on `train_data`.\n",
    "\n",
    "So, what does this mean?\n",
    "* The points with higher weights are the ones that are more important during the training process of the weighted decision tree.\n",
    "* The points with zero weights are basically ignored during training.\n",
    "\n",
    "**Quiz Question**: Will you get the same model as `small_data_decision_tree_subset_20` if you trained a decision tree with only the 20 data points with non-zero weights from the set of points in `subset_20`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your own Adaboost (on decision stumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with **decision tree stumps** by training trees with **`max_depth=1`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture the procedure for Adaboost:\n",
    "\n",
    "1\\. Start with unweighted data with $\\alpha_j = 1$\n",
    "\n",
    "2\\. For t = 1,...T:\n",
    "  * Learn $f_t(x)$ with data weights $\\alpha_j$\n",
    "  * Compute coefficient $\\hat{w}_t$:\n",
    "     $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
    "  * Re-compute weights $\\alpha_j$:\n",
    "     $$\\alpha_j \\gets \\begin{cases}\n",
    "     \\alpha_j \\exp{(-\\hat{w}_t)} & \\text{ if }f_t(x_j) = y_j\\\\\n",
    "     \\alpha_j \\exp{(\\hat{w}_t)} & \\text{ if }f_t(x_j) \\neq y_j\n",
    "     \\end{cases}$$\n",
    "  * Normalize weights $\\alpha_j$:\n",
    "      $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$\n",
    "  \n",
    "Complete the skeleton for the following code to implement **adaboost_with_tree_stumps**. Fill in the places with `YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data\n",
    "    alpha = graphlab.SArray([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in xrange(num_tree_stumps):\n",
    "        print '====================================================='\n",
    "        print 'Adaboost Iteration %d' % t\n",
    "        print '====================================================='        \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, data_weights=alpha, max_depth=1)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weighted_error = sum(alpha * is_wrong) / sum(alpha)\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weight = .5 * log((1 - weighted_error) / weighted_error)\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))\n",
    "        \n",
    "        # Scale alpha by multiplying by adjustment \n",
    "        # Then normalize data points weights\n",
    "        ## YOUR CODE HERE \n",
    "        alpha *= adjustment\n",
    "        alpha /= sum(alpha)\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking your Adaboost code\n",
    "\n",
    "Train an ensemble of **two** tree stumps and see which features those stumps split on. We will run the algorithm with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stump(tree):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('.')\n",
    "    print '                       root'\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name)))\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                 (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the first stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term. 36 months == 0]            [term. 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the next stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.A == 0]                    [grade.A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15802933659263743, 0.1768236329364191]\n"
     ]
    }
   ],
   "source": [
    "print stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Adaboost is correctly implemented, the following things should be true:\n",
    "\n",
    "* `tree_stumps[0]` should split on **term. 36 months** with the prediction -1 on the left and +1 on the right.\n",
    "* `tree_stumps[1]` should split on **grade.A** with the prediction -1 on the left and +1 on the right.\n",
    "* Weights should be approximately `[0.158, 0.177]` \n",
    "\n",
    "**Reminders**\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) and data point weights ($\\mathbf{\\alpha}$) are two different concepts.\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble.\n",
    "- Data point weights ($\\mathbf{\\alpha}$) tell you how important each data point is while training a decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a boosted ensemble of 10 stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train an ensemble of 10 decision tree stumps with Adaboost. We run the **adaboost_with_tree_stumps** function with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, ... ]\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.1788543607076036e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 2.4808968052177085e-05, 1.8086151404102345e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 1.8086151404102345e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 4.2567465809538e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 3.103239239540615e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 3.103239239540615e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.4808968052177085e-05, 1.8086151404102345e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 4.2567465809538e-05, 2.4808968052177085e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 4.2567465809538e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, ... ]\n",
      "Split on feature home_ownership.MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0310733650963335e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 1.685945653661868e-05, 2.3126297532772446e-05, 1.9505362517888515e-05, 2.7860451403095862e-05, 2.3126297532772446e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 1.9505362517888515e-05, 2.3498279556392647e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.968032356110462e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 3.968032356110462e-05, 2.0310733650963335e-05, 1.685945653661868e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 4.5907713229390207e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.968032356110462e-05, 3.2232842342757394e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.8927617552674857e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.3126297532772446e-05, 3.2232842342757394e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.8927617552674857e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 4.5907713229390207e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.3126297532772446e-05, 1.685945653661868e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 4.5907713229390207e-05, 4.5907713229390207e-05, 2.6755715173470646e-05, 2.3498279556392647e-05, 1.9505362517888515e-05, 4.5907713229390207e-05, 2.6755715173470646e-05, 2.7860451403095862e-05, 2.6755715173470646e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 2.6755715173470646e-05, 2.7860451403095862e-05, ... ]\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9036029491260022e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.611192602213042e-05, 3.454614596431647e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 1.580135495610065e-05, 2.1674888235099766e-05, 1.8281203550253472e-05, 2.9859954966688172e-05, 2.1674888235099766e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 3.0209905164836932e-05, 3.718998153944582e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 3.0209905164836932e-05, 3.718998153944582e-05, 2.611192602213042e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 2.9859954966688172e-05, 3.454614596431647e-05, 2.611192602213042e-05, 3.454614596431647e-05, 1.8281203550253472e-05, 2.518471647126885e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 3.718998153944582e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 3.718998153944582e-05, 2.1768405090908584e-05, 1.580135495610065e-05, 1.9036029491260022e-05, 1.9036029491260022e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 4.302653946080036e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 3.718998153944582e-05, 3.454614596431647e-05, 2.1768405090908584e-05, 2.9859954966688172e-05, 2.711211669197836e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.611192602213042e-05, 2.1768405090908584e-05, 2.1674888235099766e-05, 3.454614596431647e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 3.718998153944582e-05, 2.611192602213042e-05, 2.711211669197836e-05, 1.9036029491260022e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 4.302653946080036e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.1674888235099766e-05, 1.580135495610065e-05, 2.9859954966688172e-05, 2.611192602213042e-05, 4.302653946080036e-05, 4.302653946080036e-05, 2.5076523175113537e-05, 2.518471647126885e-05, 1.8281203550253472e-05, 4.302653946080036e-05, 2.5076523175113537e-05, 2.9859954966688172e-05, 2.5076523175113537e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 3.0209905164836932e-05, 2.611192602213042e-05, 2.5076523175113537e-05, 2.9859954966688172e-05, ... ]\n",
      "Split on feature grade.E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0348064240517002e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 2.791165817356422e-05, 3.692719627624719e-05, 2.791165817356422e-05, 1.7882944632693728e-05, 1.6890443770407573e-05, 2.3168803054036615e-05, 1.954121285666819e-05, 3.1918015369753364e-05, 2.3168803054036615e-05, 3.1918015369753364e-05, 1.7882944632693728e-05, 3.2292085451761906e-05, 3.9753254942987325e-05, 3.1918015369753364e-05, 2.0348064240517002e-05, 3.2292085451761906e-05, 3.9753254942987325e-05, 2.791165817356422e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 3.1918015369753364e-05, 3.692719627624719e-05, 2.791165817356422e-05, 3.692719627624719e-05, 1.954121285666819e-05, 2.692054185311439e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.9753254942987325e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 3.9753254942987325e-05, 2.3268765443275537e-05, 1.6890443770407573e-05, 1.7882944632693728e-05, 2.0348064240517002e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 4.5992090388254305e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.9753254942987325e-05, 3.692719627624719e-05, 2.3268765443275537e-05, 3.1918015369753364e-05, 2.8980785746211437e-05, 2.453022714222767e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 2.791165817356422e-05, 2.3268765443275537e-05, 2.3168803054036615e-05, 3.692719627624719e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 3.9753254942987325e-05, 2.791165817356422e-05, 2.8980785746211437e-05, 1.7882944632693728e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 1.7882944632693728e-05, 4.5992090388254305e-05, 2.791165817356422e-05, 1.7882944632693728e-05, 2.3168803054036615e-05, 1.6890443770407573e-05, 3.1918015369753364e-05, 2.791165817356422e-05, 4.5992090388254305e-05, 4.5992090388254305e-05, 2.680489146806049e-05, 2.692054185311439e-05, 1.954121285666819e-05, 4.5992090388254305e-05, 2.680489146806049e-05, 3.1918015369753364e-05, 2.680489146806049e-05, 2.0348064240517002e-05, 2.453022714222767e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 3.2292085451761906e-05, 2.791165817356422e-05, 2.680489146806049e-05, 3.1918015369753364e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.929629127644232e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 2.6468929907010296e-05, 3.501846303148477e-05, 2.6468929907010296e-05, 1.6958591462761244e-05, 1.601739206883269e-05, 2.1971228661987836e-05, 1.853114530798475e-05, 3.0268202137594344e-05, 2.1971228661987836e-05, 3.0268202137594344e-05, 1.6958591462761244e-05, 3.062293687672824e-05, 4.2044978849681946e-05, 3.0268202137594344e-05, 1.929629127644232e-05, 3.062293687672824e-05, 4.2044978849681946e-05, 2.6468929907010296e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 3.0268202137594344e-05, 3.501846303148477e-05, 2.6468929907010296e-05, 3.501846303148477e-05, 1.853114530798475e-05, 2.5529043489207745e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.2044978849681946e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 4.2044978849681946e-05, 2.2066024086095184e-05, 1.601739206883269e-05, 1.6958591462761244e-05, 1.929629127644232e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.8643475116694395e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.2044978849681946e-05, 3.501846303148477e-05, 2.2066024086095184e-05, 3.0268202137594344e-05, 3.0651490689105764e-05, 2.3262281975265167e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 2.6468929907010296e-05, 2.2066024086095184e-05, 2.1971228661987836e-05, 3.501846303148477e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 4.2044978849681946e-05, 2.6468929907010296e-05, 3.0651490689105764e-05, 1.6958591462761244e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 1.6958591462761244e-05, 4.8643475116694395e-05, 2.6468929907010296e-05, 1.6958591462761244e-05, 2.1971228661987836e-05, 1.601739206883269e-05, 3.0268202137594344e-05, 2.6468929907010296e-05, 4.8643475116694395e-05, 4.8643475116694395e-05, 2.5419370967544032e-05, 2.5529043489207745e-05, 1.853114530798475e-05, 4.8643475116694395e-05, 2.5419370967544032e-05, 3.0268202137594344e-05, 2.5419370967544032e-05, 1.929629127644232e-05, 2.3262281975265167e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 3.062293687672824e-05, 2.6468929907010296e-05, 2.5419370967544032e-05, 3.0268202137594344e-05, ... ]\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0173505904006534e-05, 1.8492186308167507e-05, 2.3069151517330548e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 2.7672214629331413e-05, 3.6610411845168866e-05, 2.7672214629331413e-05, 1.7729533623661425e-05, 1.674554705037387e-05, 2.2970046667569232e-05, 1.937357619259354e-05, 3.1644202804498845e-05, 2.2970046667569232e-05, 3.1644202804498845e-05, 1.7729533623661425e-05, 3.201506388094883e-05, 4.3956354975497195e-05, 3.1644202804498845e-05, 2.0173505904006534e-05, 3.201506388094883e-05, 4.3956354975497195e-05, 2.7672214629331413e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 3.1644202804498845e-05, 3.6610411845168866e-05, 2.7672214629331413e-05, 3.6610411845168866e-05, 1.937357619259354e-05, 2.668960072042015e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 4.3956354975497195e-05, 2.0173505904006534e-05, 2.3069151517330548e-05, 4.3956354975497195e-05, 2.3069151517330548e-05, 1.674554705037387e-05, 1.7729533623661425e-05, 2.0173505904006534e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 5.085482066992069e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 4.3956354975497195e-05, 3.6610411845168866e-05, 2.3069151517330548e-05, 3.1644202804498845e-05, 3.204491575737045e-05, 2.431979161413232e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 2.7672214629331413e-05, 2.3069151517330548e-05, 2.2970046667569232e-05, 3.6610411845168866e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 4.3956354975497195e-05, 2.7672214629331413e-05, 3.204491575737045e-05, 1.7729533623661425e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 1.7729533623661425e-05, 5.085482066992069e-05, 2.7672214629331413e-05, 1.7729533623661425e-05, 2.2970046667569232e-05, 1.674554705037387e-05, 3.1644202804498845e-05, 2.7672214629331413e-05, 5.085482066992069e-05, 5.085482066992069e-05, 2.65749424562183e-05, 2.668960072042015e-05, 1.937357619259354e-05, 5.085482066992069e-05, 2.65749424562183e-05, 3.1644202804498845e-05, 2.65749424562183e-05, 1.8492186308167507e-05, 2.431979161413232e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 2.0173505904006534e-05, 2.3069151517330548e-05, 3.201506388094883e-05, 2.7672214629331413e-05, 2.65749424562183e-05, 3.1644202804498845e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9605331743486973e-05, 1.7971365461170967e-05, 2.2419423311453615e-05, 2.689284403348397e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 2.689284403348397e-05, 3.557930252211e-05, 2.689284403348397e-05, 1.7230192411927546e-05, 1.6273919204274127e-05, 2.2323109687723337e-05, 1.882793155145546e-05, 3.075296364907836e-05, 2.2323109687723337e-05, 3.075296364907836e-05, 1.7230192411927546e-05, 3.111337965555469e-05, 4.526825592387652e-05, 3.075296364907836e-05, 1.9605331743486973e-05, 3.111337965555469e-05, 4.526825592387652e-05, 2.689284403348397e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 3.075296364907836e-05, 3.557930252211e-05, 2.689284403348397e-05, 3.557930252211e-05, 1.882793155145546e-05, 2.593790482997429e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 4.526825592387652e-05, 1.9605331743486973e-05, 2.2419423311453615e-05, 4.526825592387652e-05, 2.2419423311453615e-05, 1.6273919204274127e-05, 1.7230192411927546e-05, 1.9605331743486973e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 5.237261002037342e-05, 3.075296364907836e-05, 3.075296364907836e-05, 4.526825592387652e-05, 3.557930252211e-05, 2.2419423311453615e-05, 3.075296364907836e-05, 3.3001313424926466e-05, 2.363483991311807e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 2.689284403348397e-05, 2.2419423311453615e-05, 2.2323109687723337e-05, 3.557930252211e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 4.526825592387652e-05, 2.689284403348397e-05, 3.3001313424926466e-05, 1.7230192411927546e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 3.075296364907836e-05, 1.7230192411927546e-05, 5.237261002037342e-05, 2.689284403348397e-05, 1.7230192411927546e-05, 2.2323109687723337e-05, 1.6273919204274127e-05, 3.075296364907836e-05, 2.689284403348397e-05, 5.237261002037342e-05, 5.237261002037342e-05, 2.5826475843981168e-05, 2.593790482997429e-05, 1.882793155145546e-05, 5.237261002037342e-05, 2.5826475843981168e-05, 3.075296364907836e-05, 2.5826475843981168e-05, 1.7971365461170967e-05, 2.363483991311807e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 3.075296364907836e-05, 1.9605331743486973e-05, 2.2419423311453615e-05, 3.111337965555469e-05, 2.689284403348397e-05, 2.5826475843981168e-05, 3.075296364907836e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
    "                                target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Recall from the lecture that in order to make predictions, we use the following formula:\n",
    "$$\n",
    "\\hat{y} = sign\\left(\\sum_{t=1}^T \\hat{w}_t f_t(x)\\right)\n",
    "$$\n",
    "\n",
    "We need to do the following things:\n",
    "- Compute the predictions $f_t(x)$ using the $t$-th decision tree\n",
    "- Compute $\\hat{w}_t f_t(x)$ by multiplying the `stump_weights` with the predictions $f_t(x)$ from the decision trees\n",
    "- Sum the weighted predictions over each stump in the ensemble.\n",
    "\n",
    "Complete the following skeleton for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = graphlab.SArray([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        \n",
    "        # Accumulate predictions on scaores array\n",
    "        # YOUR CODE HERE\n",
    "        scores += stump_weights[i] * predictions\n",
    "        \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10-component ensemble = 0.620314519604\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
    "accuracy = graphlab.evaluation.accuracy(test_data[target], predictions)\n",
    "print 'Accuracy of 10-component ensemble = %s' % accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us take a quick look what the `stump_weights` look like at the end of each iteration of the 10-stump ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15802933659263743,\n",
       " 0.1768236329364191,\n",
       " 0.09311888971129693,\n",
       " 0.07288885525840554,\n",
       " 0.06706306914118143,\n",
       " 0.06456916961644447,\n",
       " 0.05456055779178564,\n",
       " 0.04351093673362621,\n",
       " 0.02898871150041245,\n",
       " 0.02596250969152032]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Are the weights monotonically decreasing, monotonically increasing, or neither?\n",
    "\n",
    "**Reminder**: Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance plots\n",
    "\n",
    "In this section, we will try to reproduce some of the performance plots dicussed in the lecture.\n",
    "\n",
    "### How does accuracy change with adding stumps to the ensemble?\n",
    "\n",
    "We will now train an ensemble with:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 30`\n",
    "\n",
    "Once we are done with this, we will then do the following:\n",
    "* Compute the classification error at the end of each iteration.\n",
    "* Plot a curve of classification error vs iteration.\n",
    "\n",
    "First, lets train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, ... ]\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.1788543607076036e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 2.4808968052177085e-05, 1.8086151404102345e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 1.8086151404102345e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 4.2567465809538e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 3.103239239540615e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 3.103239239540615e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.4808968052177085e-05, 1.8086151404102345e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 4.2567465809538e-05, 2.4808968052177085e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 4.2567465809538e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, ... ]\n",
      "Split on feature home_ownership.MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0310733650963335e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 1.685945653661868e-05, 2.3126297532772446e-05, 1.9505362517888515e-05, 2.7860451403095862e-05, 2.3126297532772446e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 1.9505362517888515e-05, 2.3498279556392647e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.968032356110462e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 3.968032356110462e-05, 2.0310733650963335e-05, 1.685945653661868e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 4.5907713229390207e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.968032356110462e-05, 3.2232842342757394e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.8927617552674857e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.3126297532772446e-05, 3.2232842342757394e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.8927617552674857e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 4.5907713229390207e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.3126297532772446e-05, 1.685945653661868e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 4.5907713229390207e-05, 4.5907713229390207e-05, 2.6755715173470646e-05, 2.3498279556392647e-05, 1.9505362517888515e-05, 4.5907713229390207e-05, 2.6755715173470646e-05, 2.7860451403095862e-05, 2.6755715173470646e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 2.6755715173470646e-05, 2.7860451403095862e-05, ... ]\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9036029491260022e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.611192602213042e-05, 3.454614596431647e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 1.580135495610065e-05, 2.1674888235099766e-05, 1.8281203550253472e-05, 2.9859954966688172e-05, 2.1674888235099766e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 3.0209905164836932e-05, 3.718998153944582e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 3.0209905164836932e-05, 3.718998153944582e-05, 2.611192602213042e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 2.9859954966688172e-05, 3.454614596431647e-05, 2.611192602213042e-05, 3.454614596431647e-05, 1.8281203550253472e-05, 2.518471647126885e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 3.718998153944582e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 3.718998153944582e-05, 2.1768405090908584e-05, 1.580135495610065e-05, 1.9036029491260022e-05, 1.9036029491260022e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 4.302653946080036e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 3.718998153944582e-05, 3.454614596431647e-05, 2.1768405090908584e-05, 2.9859954966688172e-05, 2.711211669197836e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.611192602213042e-05, 2.1768405090908584e-05, 2.1674888235099766e-05, 3.454614596431647e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 3.718998153944582e-05, 2.611192602213042e-05, 2.711211669197836e-05, 1.9036029491260022e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 4.302653946080036e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.1674888235099766e-05, 1.580135495610065e-05, 2.9859954966688172e-05, 2.611192602213042e-05, 4.302653946080036e-05, 4.302653946080036e-05, 2.5076523175113537e-05, 2.518471647126885e-05, 1.8281203550253472e-05, 4.302653946080036e-05, 2.5076523175113537e-05, 2.9859954966688172e-05, 2.5076523175113537e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 3.0209905164836932e-05, 2.611192602213042e-05, 2.5076523175113537e-05, 2.9859954966688172e-05, ... ]\n",
      "Split on feature grade.E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0348064240517002e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 2.791165817356422e-05, 3.692719627624719e-05, 2.791165817356422e-05, 1.7882944632693728e-05, 1.6890443770407573e-05, 2.3168803054036615e-05, 1.954121285666819e-05, 3.1918015369753364e-05, 2.3168803054036615e-05, 3.1918015369753364e-05, 1.7882944632693728e-05, 3.2292085451761906e-05, 3.9753254942987325e-05, 3.1918015369753364e-05, 2.0348064240517002e-05, 3.2292085451761906e-05, 3.9753254942987325e-05, 2.791165817356422e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 3.1918015369753364e-05, 3.692719627624719e-05, 2.791165817356422e-05, 3.692719627624719e-05, 1.954121285666819e-05, 2.692054185311439e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.9753254942987325e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 3.9753254942987325e-05, 2.3268765443275537e-05, 1.6890443770407573e-05, 1.7882944632693728e-05, 2.0348064240517002e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 4.5992090388254305e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.9753254942987325e-05, 3.692719627624719e-05, 2.3268765443275537e-05, 3.1918015369753364e-05, 2.8980785746211437e-05, 2.453022714222767e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 2.791165817356422e-05, 2.3268765443275537e-05, 2.3168803054036615e-05, 3.692719627624719e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 3.9753254942987325e-05, 2.791165817356422e-05, 2.8980785746211437e-05, 1.7882944632693728e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 1.7882944632693728e-05, 4.5992090388254305e-05, 2.791165817356422e-05, 1.7882944632693728e-05, 2.3168803054036615e-05, 1.6890443770407573e-05, 3.1918015369753364e-05, 2.791165817356422e-05, 4.5992090388254305e-05, 4.5992090388254305e-05, 2.680489146806049e-05, 2.692054185311439e-05, 1.954121285666819e-05, 4.5992090388254305e-05, 2.680489146806049e-05, 3.1918015369753364e-05, 2.680489146806049e-05, 2.0348064240517002e-05, 2.453022714222767e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 3.2292085451761906e-05, 2.791165817356422e-05, 2.680489146806049e-05, 3.1918015369753364e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.929629127644232e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 2.6468929907010296e-05, 3.501846303148477e-05, 2.6468929907010296e-05, 1.6958591462761244e-05, 1.601739206883269e-05, 2.1971228661987836e-05, 1.853114530798475e-05, 3.0268202137594344e-05, 2.1971228661987836e-05, 3.0268202137594344e-05, 1.6958591462761244e-05, 3.062293687672824e-05, 4.2044978849681946e-05, 3.0268202137594344e-05, 1.929629127644232e-05, 3.062293687672824e-05, 4.2044978849681946e-05, 2.6468929907010296e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 3.0268202137594344e-05, 3.501846303148477e-05, 2.6468929907010296e-05, 3.501846303148477e-05, 1.853114530798475e-05, 2.5529043489207745e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.2044978849681946e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 4.2044978849681946e-05, 2.2066024086095184e-05, 1.601739206883269e-05, 1.6958591462761244e-05, 1.929629127644232e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.8643475116694395e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.2044978849681946e-05, 3.501846303148477e-05, 2.2066024086095184e-05, 3.0268202137594344e-05, 3.0651490689105764e-05, 2.3262281975265167e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 2.6468929907010296e-05, 2.2066024086095184e-05, 2.1971228661987836e-05, 3.501846303148477e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 4.2044978849681946e-05, 2.6468929907010296e-05, 3.0651490689105764e-05, 1.6958591462761244e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 1.6958591462761244e-05, 4.8643475116694395e-05, 2.6468929907010296e-05, 1.6958591462761244e-05, 2.1971228661987836e-05, 1.601739206883269e-05, 3.0268202137594344e-05, 2.6468929907010296e-05, 4.8643475116694395e-05, 4.8643475116694395e-05, 2.5419370967544032e-05, 2.5529043489207745e-05, 1.853114530798475e-05, 4.8643475116694395e-05, 2.5419370967544032e-05, 3.0268202137594344e-05, 2.5419370967544032e-05, 1.929629127644232e-05, 2.3262281975265167e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 3.062293687672824e-05, 2.6468929907010296e-05, 2.5419370967544032e-05, 3.0268202137594344e-05, ... ]\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0173505904006534e-05, 1.8492186308167507e-05, 2.3069151517330548e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 2.7672214629331413e-05, 3.6610411845168866e-05, 2.7672214629331413e-05, 1.7729533623661425e-05, 1.674554705037387e-05, 2.2970046667569232e-05, 1.937357619259354e-05, 3.1644202804498845e-05, 2.2970046667569232e-05, 3.1644202804498845e-05, 1.7729533623661425e-05, 3.201506388094883e-05, 4.3956354975497195e-05, 3.1644202804498845e-05, 2.0173505904006534e-05, 3.201506388094883e-05, 4.3956354975497195e-05, 2.7672214629331413e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 3.1644202804498845e-05, 3.6610411845168866e-05, 2.7672214629331413e-05, 3.6610411845168866e-05, 1.937357619259354e-05, 2.668960072042015e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 4.3956354975497195e-05, 2.0173505904006534e-05, 2.3069151517330548e-05, 4.3956354975497195e-05, 2.3069151517330548e-05, 1.674554705037387e-05, 1.7729533623661425e-05, 2.0173505904006534e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 5.085482066992069e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 4.3956354975497195e-05, 3.6610411845168866e-05, 2.3069151517330548e-05, 3.1644202804498845e-05, 3.204491575737045e-05, 2.431979161413232e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 2.7672214629331413e-05, 2.3069151517330548e-05, 2.2970046667569232e-05, 3.6610411845168866e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 4.3956354975497195e-05, 2.7672214629331413e-05, 3.204491575737045e-05, 1.7729533623661425e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 1.7729533623661425e-05, 5.085482066992069e-05, 2.7672214629331413e-05, 1.7729533623661425e-05, 2.2970046667569232e-05, 1.674554705037387e-05, 3.1644202804498845e-05, 2.7672214629331413e-05, 5.085482066992069e-05, 5.085482066992069e-05, 2.65749424562183e-05, 2.668960072042015e-05, 1.937357619259354e-05, 5.085482066992069e-05, 2.65749424562183e-05, 3.1644202804498845e-05, 2.65749424562183e-05, 1.8492186308167507e-05, 2.431979161413232e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 2.0173505904006534e-05, 2.3069151517330548e-05, 3.201506388094883e-05, 2.7672214629331413e-05, 2.65749424562183e-05, 3.1644202804498845e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9605331743486973e-05, 1.7971365461170967e-05, 2.2419423311453615e-05, 2.689284403348397e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 2.689284403348397e-05, 3.557930252211e-05, 2.689284403348397e-05, 1.7230192411927546e-05, 1.6273919204274127e-05, 2.2323109687723337e-05, 1.882793155145546e-05, 3.075296364907836e-05, 2.2323109687723337e-05, 3.075296364907836e-05, 1.7230192411927546e-05, 3.111337965555469e-05, 4.526825592387652e-05, 3.075296364907836e-05, 1.9605331743486973e-05, 3.111337965555469e-05, 4.526825592387652e-05, 2.689284403348397e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 3.075296364907836e-05, 3.557930252211e-05, 2.689284403348397e-05, 3.557930252211e-05, 1.882793155145546e-05, 2.593790482997429e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 4.526825592387652e-05, 1.9605331743486973e-05, 2.2419423311453615e-05, 4.526825592387652e-05, 2.2419423311453615e-05, 1.6273919204274127e-05, 1.7230192411927546e-05, 1.9605331743486973e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 5.237261002037342e-05, 3.075296364907836e-05, 3.075296364907836e-05, 4.526825592387652e-05, 3.557930252211e-05, 2.2419423311453615e-05, 3.075296364907836e-05, 3.3001313424926466e-05, 2.363483991311807e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 2.689284403348397e-05, 2.2419423311453615e-05, 2.2323109687723337e-05, 3.557930252211e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 4.526825592387652e-05, 2.689284403348397e-05, 3.3001313424926466e-05, 1.7230192411927546e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 3.075296364907836e-05, 1.7230192411927546e-05, 5.237261002037342e-05, 2.689284403348397e-05, 1.7230192411927546e-05, 2.2323109687723337e-05, 1.6273919204274127e-05, 3.075296364907836e-05, 2.689284403348397e-05, 5.237261002037342e-05, 5.237261002037342e-05, 2.5826475843981168e-05, 2.593790482997429e-05, 1.882793155145546e-05, 5.237261002037342e-05, 2.5826475843981168e-05, 3.075296364907836e-05, 2.5826475843981168e-05, 1.7971365461170967e-05, 2.363483991311807e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 3.075296364907836e-05, 1.9605331743486973e-05, 2.2419423311453615e-05, 3.111337965555469e-05, 2.689284403348397e-05, 2.5826475843981168e-05, 3.075296364907836e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.012778210039538e-05, 1.8450273261465618e-05, 2.3016864653635486e-05, 2.760949479703167e-05, 3.157248035222162e-05, 3.157248035222162e-05, 3.157248035222162e-05, 2.760949479703167e-05, 3.652743334409438e-05, 2.760949479703167e-05, 1.7689349150155283e-05, 1.670759280938393e-05, 2.291798442773028e-05, 1.93296654515805e-05, 3.157248035222162e-05, 2.291798442773028e-05, 3.157248035222162e-05, 1.7689349150155283e-05, 3.1942500861885285e-05, 4.647458167105024e-05, 3.157248035222162e-05, 2.012778210039538e-05, 3.1942500861885285e-05, 4.4122970198274376e-05, 2.760949479703167e-05, 2.291798442773028e-05, 2.291798442773028e-05, 2.291798442773028e-05, 3.157248035222162e-05, 3.652743334409438e-05, 2.760949479703167e-05, 3.652743334409438e-05, 1.93296654515805e-05, 2.662910800945521e-05, 3.157248035222162e-05, 3.157248035222162e-05, 3.157248035222162e-05, 4.647458167105024e-05, 2.012778210039538e-05, 2.3016864653635486e-05, 4.647458167105024e-05, 2.3016864653635486e-05, 1.670759280938393e-05, 1.7689349150155283e-05, 2.012778210039538e-05, 3.157248035222162e-05, 3.157248035222162e-05, 3.157248035222162e-05, 5.376825530479759e-05, 3.157248035222162e-05, 3.157248035222162e-05, 4.647458167105024e-05, 3.652743334409438e-05, 2.3016864653635486e-05, 3.157248035222162e-05, 3.388074501031787e-05, 2.426467014040733e-05, 2.012778210039538e-05, 2.621245576219321e-05, 2.760949479703167e-05, 2.3016864653635486e-05, 2.291798442773028e-05, 3.652743334409438e-05, 2.291798442773028e-05, 2.291798442773028e-05, 4.647458167105024e-05, 2.760949479703167e-05, 3.388074501031787e-05, 1.7689349150155283e-05, 2.012778210039538e-05, 2.760949479703167e-05, 3.157248035222162e-05, 1.7689349150155283e-05, 5.104758431649547e-05, 2.760949479703167e-05, 1.7689349150155283e-05, 2.291798442773028e-05, 1.670759280938393e-05, 3.157248035222162e-05, 2.760949479703167e-05, 5.376825530479759e-05, 5.376825530479759e-05, 2.517306666081184e-05, 2.662910800945521e-05, 1.93296654515805e-05, 5.376825530479759e-05, 2.6514709621349248e-05, 3.157248035222162e-05, 2.6514709621349248e-05, 1.8450273261465618e-05, 2.426467014040733e-05, 2.012778210039538e-05, 2.760949479703167e-05, 3.157248035222162e-05, 2.012778210039538e-05, 2.3016864653635486e-05, 3.1942500861885285e-05, 2.760949479703167e-05, 2.6514709621349248e-05, 3.157248035222162e-05, ... ]\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9527038425207157e-05, 1.7899597339397576e-05, 2.232989200089282e-05, 2.6785448248250064e-05, 3.063015258918263e-05, 3.063015258918263e-05, 3.063015258918263e-05, 2.6785448248250064e-05, 3.543721761923847e-05, 2.6785448248250064e-05, 1.7161384143025e-05, 1.7237912179357335e-05, 2.3645429200980195e-05, 1.9943212604724033e-05, 3.063015258918263e-05, 2.3645429200980195e-05, 3.063015258918263e-05, 1.7161384143025e-05, 3.0989129284889974e-05, 4.508747846928448e-05, 3.063015258918263e-05, 1.9527038425207157e-05, 3.0989129284889974e-05, 4.2806054348086825e-05, 2.6785448248250064e-05, 2.3645429200980195e-05, 2.3645429200980195e-05, 2.3645429200980195e-05, 3.063015258918263e-05, 3.543721761923847e-05, 2.6785448248250064e-05, 3.543721761923847e-05, 1.9943212604724033e-05, 2.5834322566489284e-05, 3.063015258918263e-05, 3.063015258918263e-05, 3.063015258918263e-05, 4.508747846928448e-05, 1.9527038425207157e-05, 2.232989200089282e-05, 4.508747846928448e-05, 2.232989200089282e-05, 1.7237912179357335e-05, 1.7161384143025e-05, 1.9527038425207157e-05, 3.063015258918263e-05, 3.063015258918263e-05, 3.063015258918263e-05, 5.216346153571021e-05, 3.063015258918263e-05, 3.063015258918263e-05, 4.508747846928448e-05, 3.543721761923847e-05, 2.232989200089282e-05, 3.063015258918263e-05, 3.286952364603185e-05, 2.3540454871945538e-05, 1.9527038425207157e-05, 2.5430105926938413e-05, 2.6785448248250064e-05, 2.232989200089282e-05, 2.3645429200980195e-05, 3.543721761923847e-05, 2.3645429200980195e-05, 2.3645429200980195e-05, 4.508747846928448e-05, 2.6785448248250064e-05, 3.286952364603185e-05, 1.7161384143025e-05, 1.9527038425207157e-05, 2.6785448248250064e-05, 3.063015258918263e-05, 1.7161384143025e-05, 4.9523993030639394e-05, 2.6785448248250064e-05, 1.7161384143025e-05, 2.3645429200980195e-05, 1.7237912179357335e-05, 3.063015258918263e-05, 2.6785448248250064e-05, 5.216346153571021e-05, 5.216346153571021e-05, 2.5972090494117277e-05, 2.5834322566489284e-05, 1.9943212604724033e-05, 5.216346153571021e-05, 2.735631883829905e-05, 3.063015258918263e-05, 2.735631883829905e-05, 1.7899597339397576e-05, 2.3540454871945538e-05, 1.9527038425207157e-05, 2.6785448248250064e-05, 3.063015258918263e-05, 1.9527038425207157e-05, 2.232989200089282e-05, 3.0989129284889974e-05, 2.6785448248250064e-05, 2.735631883829905e-05, 3.063015258918263e-05, ... ]\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9061324098719876e-05, 1.7472697021090725e-05, 2.28891288713697e-05, 2.6146622906743554e-05, 3.139726380832865e-05, 3.139726380832865e-05, 3.139726380832865e-05, 2.6146622906743554e-05, 3.632471849380601e-05, 2.6146622906743554e-05, 1.675209000001552e-05, 1.682679286439285e-05, 2.3081492422906265e-05, 1.9467572642126726e-05, 3.139726380832865e-05, 2.3081492422906265e-05, 3.139726380832865e-05, 1.675209000001552e-05, 3.025004734327257e-05, 4.4012154899417035e-05, 3.139726380832865e-05, 1.9061324098719876e-05, 3.025004734327257e-05, 4.1785142096254374e-05, 2.6146622906743554e-05, 2.3081492422906265e-05, 2.3081492422906265e-05, 2.3081492422906265e-05, 3.139726380832865e-05, 3.632471849380601e-05, 2.6146622906743554e-05, 3.632471849380601e-05, 1.9467572642126726e-05, 2.6481325503287904e-05, 3.139726380832865e-05, 3.139726380832865e-05, 3.139726380832865e-05, 4.4012154899417035e-05, 1.9061324098719876e-05, 2.28891288713697e-05, 4.4012154899417035e-05, 2.28891288713697e-05, 1.682679286439285e-05, 1.675209000001552e-05, 1.9061324098719876e-05, 3.139726380832865e-05, 3.139726380832865e-05, 3.139726380832865e-05, 5.091937777721315e-05, 3.139726380832865e-05, 3.139726380832865e-05, 4.4012154899417035e-05, 3.632471849380601e-05, 2.28891288713697e-05, 3.139726380832865e-05, 3.208559483238191e-05, 2.2979021701837154e-05, 1.9061324098719876e-05, 2.4823605115275334e-05, 2.6146622906743554e-05, 2.28891288713697e-05, 2.3081492422906265e-05, 3.632471849380601e-05, 2.3081492422906265e-05, 2.3081492422906265e-05, 4.4012154899417035e-05, 2.6146622906743554e-05, 3.208559483238191e-05, 1.675209000001552e-05, 1.9061324098719876e-05, 2.6146622906743554e-05, 3.139726380832865e-05, 1.675209000001552e-05, 4.834285984715306e-05, 2.6146622906743554e-05, 1.675209000001552e-05, 2.3081492422906265e-05, 1.682679286439285e-05, 3.139726380832865e-05, 2.6146622906743554e-05, 5.091937777721315e-05, 5.091937777721315e-05, 2.5352663504291698e-05, 2.6481325503287904e-05, 1.9467572642126726e-05, 5.091937777721315e-05, 2.6703878395179752e-05, 3.139726380832865e-05, 2.6703878395179752e-05, 1.7472697021090725e-05, 2.2979021701837154e-05, 1.9061324098719876e-05, 2.6146622906743554e-05, 3.139726380832865e-05, 1.9061324098719876e-05, 2.28891288713697e-05, 3.025004734327257e-05, 2.6146622906743554e-05, 2.6703878395179752e-05, 3.139726380832865e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9601149594862333e-05, 1.796753186517118e-05, 2.3537359565379334e-05, 2.6887107335316405e-05, 3.228644881064361e-05, 3.228644881064361e-05, 3.228644881064361e-05, 2.6887107335316405e-05, 3.7353451287057806e-05, 2.6887107335316405e-05, 1.7226516920666262e-05, 1.7303335404641523e-05, 2.373517094147993e-05, 2.0018903284520614e-05, 3.228644881064361e-05, 2.373517094147993e-05, 3.228644881064361e-05, 1.7226516920666262e-05, 3.110674264580465e-05, 4.5258599439776746e-05, 3.228644881064361e-05, 1.9601149594862333e-05, 3.110674264580465e-05, 4.0665202025748647e-05, 2.6887107335316405e-05, 2.373517094147993e-05, 2.373517094147993e-05, 2.373517094147993e-05, 3.228644881064361e-05, 3.7353451287057806e-05, 2.6887107335316405e-05, 3.7353451287057806e-05, 2.0018903284520614e-05, 2.723128886387534e-05, 3.228644881064361e-05, 3.228644881064361e-05, 3.228644881064361e-05, 4.5258599439776746e-05, 1.9601149594862333e-05, 2.3537359565379334e-05, 4.5258599439776746e-05, 2.3537359565379334e-05, 1.7303335404641523e-05, 1.7226516920666262e-05, 1.9601149594862333e-05, 3.228644881064361e-05, 3.228644881064361e-05, 3.228644881064361e-05, 5.2361438057469085e-05, 3.228644881064361e-05, 3.228644881064361e-05, 4.5258599439776746e-05, 3.7353451287057806e-05, 2.3537359565379334e-05, 3.228644881064361e-05, 3.299427368699409e-05, 2.362979820229525e-05, 1.9601149594862333e-05, 2.4158274122766876e-05, 2.6887107335316405e-05, 2.3537359565379334e-05, 2.373517094147993e-05, 3.7353451287057806e-05, 2.373517094147993e-05, 2.373517094147993e-05, 4.5258599439776746e-05, 2.6887107335316405e-05, 3.299427368699409e-05, 1.7226516920666262e-05, 1.9601149594862333e-05, 2.6887107335316405e-05, 3.228644881064361e-05, 1.7226516920666262e-05, 4.704715751973361e-05, 2.6887107335316405e-05, 1.7226516920666262e-05, 2.373517094147993e-05, 1.7303335404641523e-05, 3.228644881064361e-05, 2.6887107335316405e-05, 5.2361438057469085e-05, 5.2361438057469085e-05, 2.4673152502819007e-05, 2.723128886387534e-05, 2.0018903284520614e-05, 5.2361438057469085e-05, 2.7460144556383836e-05, 3.228644881064361e-05, 2.7460144556383836e-05, 1.796753186517118e-05, 2.362979820229525e-05, 1.9601149594862333e-05, 2.6887107335316405e-05, 3.228644881064361e-05, 1.9601149594862333e-05, 2.3537359565379334e-05, 3.110674264580465e-05, 2.6887107335316405e-05, 2.7460144556383836e-05, 3.228644881064361e-05, ... ]\n",
      "Split on feature emp_length.4 years. (34593, 2631)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34593 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2631 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9175991293896658e-05, 1.8374929516427492e-05, 2.302682298926658e-05, 2.6303913129423442e-05, 3.158614030811851e-05, 3.158614030811851e-05, 3.158614030811851e-05, 2.6303913129423442e-05, 3.654323707959253e-05, 2.7496748351104293e-05, 1.6852865537103196e-05, 1.6928017791453933e-05, 2.3220343742096962e-05, 1.9584683706405975e-05, 3.158614030811851e-05, 2.3220343742096962e-05, 3.3018515046093585e-05, 1.6852865537103196e-05, 3.0432022533708094e-05, 4.4276918791466954e-05, 3.158614030811851e-05, 1.9175991293896658e-05, 3.0432022533708094e-05, 3.978315436226747e-05, 2.6303913129423442e-05, 2.3220343742096962e-05, 2.3220343742096962e-05, 2.3220343742096962e-05, 3.158614030811851e-05, 3.654323707959253e-05, 2.6303913129423442e-05, 3.654323707959253e-05, 1.9584683706405975e-05, 2.6640629196163533e-05, 3.158614030811851e-05, 3.158614030811851e-05, 3.158614030811851e-05, 4.4276918791466954e-05, 1.9175991293896658e-05, 2.302682298926658e-05, 4.4276918791466954e-05, 2.302682298926658e-05, 1.6928017791453933e-05, 1.6852865537103196e-05, 1.9175991293896658e-05, 3.158614030811851e-05, 3.158614030811851e-05, 3.158614030811851e-05, 5.1225693445506724e-05, 3.158614030811851e-05, 3.158614030811851e-05, 4.62847975425143e-05, 3.654323707959253e-05, 2.302682298926658e-05, 3.158614030811851e-05, 3.227861212467247e-05, 2.3117256587976722e-05, 1.9175991293896658e-05, 2.363426961320531e-05, 2.6303913129423442e-05, 2.302682298926658e-05, 2.3220343742096962e-05, 3.654323707959253e-05, 2.3220343742096962e-05, 2.3220343742096962e-05, 4.4276918791466954e-05, 2.6303913129423442e-05, 3.227861212467247e-05, 1.6852865537103196e-05, 1.9175991293896658e-05, 2.6303913129423442e-05, 3.158614030811851e-05, 1.6852865537103196e-05, 4.6026682192021334e-05, 2.6303913129423442e-05, 1.6852865537103196e-05, 2.3220343742096962e-05, 1.6928017791453933e-05, 3.158614030811851e-05, 2.6303913129423442e-05, 5.354868664793165e-05, 5.1225693445506724e-05, 2.4137980035163584e-05, 2.6640629196163533e-05, 1.9584683706405975e-05, 5.354868664793165e-05, 2.686452089934465e-05, 3.158614030811851e-05, 2.8082778676605722e-05, 1.8374929516427492e-05, 2.3117256587976722e-05, 1.9175991293896658e-05, 2.6303913129423442e-05, 3.158614030811851e-05, 1.9175991293896658e-05, 2.302682298926658e-05, 3.0432022533708094e-05, 2.6303913129423442e-05, 2.686452089934465e-05, 3.158614030811851e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9514445739805173e-05, 1.8699245297175155e-05, 2.3433244253044806e-05, 2.6768174726490307e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 2.6768174726490307e-05, 3.718822178301245e-05, 2.798206337783884e-05, 1.7150317031521973e-05, 1.7226795716105972e-05, 2.363018062899303e-05, 1.9930351535023256e-05, 3.2143632718942425e-05, 2.363018062899303e-05, 3.3601288736557476e-05, 1.7150317031521973e-05, 3.0969144874173484e-05, 4.505840225098813e-05, 3.2143632718942425e-05, 1.9514445739805173e-05, 3.0969144874173484e-05, 3.910492673100485e-05, 2.6768174726490307e-05, 2.363018062899303e-05, 2.363018062899303e-05, 2.363018062899303e-05, 3.2143632718942425e-05, 3.718822178301245e-05, 2.6768174726490307e-05, 3.718822178301245e-05, 1.9930351535023256e-05, 2.711083380019418e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 4.505840225098813e-05, 1.9514445739805173e-05, 2.3433244253044806e-05, 4.505840225098813e-05, 2.3433244253044806e-05, 1.7226795716105972e-05, 1.7150317031521973e-05, 1.9514445739805173e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 5.212982212525309e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 4.7101719873472256e-05, 3.718822178301245e-05, 2.3433244253044806e-05, 3.2143632718942425e-05, 3.2848326598042564e-05, 2.3525273996281395e-05, 1.9514445739805173e-05, 2.3231349961575335e-05, 2.6768174726490307e-05, 2.3433244253044806e-05, 2.363018062899303e-05, 3.718822178301245e-05, 2.363018062899303e-05, 2.363018062899303e-05, 4.505840225098813e-05, 2.6768174726490307e-05, 3.2848326598042564e-05, 1.7150317031521973e-05, 1.9514445739805173e-05, 2.6768174726490307e-05, 3.2143632718942425e-05, 1.7150317031521973e-05, 4.524201420532243e-05, 2.6768174726490307e-05, 1.7150317031521973e-05, 2.363018062899303e-05, 1.7226795716105972e-05, 3.2143632718942425e-05, 2.6768174726490307e-05, 5.4493815939595594e-05, 5.212982212525309e-05, 2.372647307235119e-05, 2.711083380019418e-05, 1.9930351535023256e-05, 5.4493815939595594e-05, 2.7338677170915308e-05, 3.2143632718942425e-05, 2.8578437083563136e-05, 1.8699245297175155e-05, 2.3525273996281395e-05, 1.9514445739805173e-05, 2.6768174726490307e-05, 3.2143632718942425e-05, 1.9514445739805173e-05, 2.3433244253044806e-05, 3.0969144874173484e-05, 2.6768174726490307e-05, 2.7338677170915308e-05, 3.2143632718942425e-05, ... ]\n",
      "Split on feature grade.C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.98991302647242e-05, 1.8344612287159934e-05, 2.2988830491322855e-05, 2.729585061925754e-05, 3.153402559080532e-05, 3.153402559080532e-05, 3.153402559080532e-05, 2.729585061925754e-05, 3.648294353148774e-05, 2.853366842469914e-05, 1.6825059597066972e-05, 1.690008785594207e-05, 2.3182031949702262e-05, 1.955237047518901e-05, 3.153402559080532e-05, 2.3182031949702262e-05, 3.296403701994215e-05, 1.6825059597066972e-05, 3.157963368548363e-05, 4.4203865259639156e-05, 3.153402559080532e-05, 1.98991302647242e-05, 3.157963368548363e-05, 3.8363297983285506e-05, 2.729585061925754e-05, 2.3182031949702262e-05, 2.3182031949702262e-05, 2.3182031949702262e-05, 3.153402559080532e-05, 3.648294353148774e-05, 2.729585061925754e-05, 3.648294353148774e-05, 1.955237047518901e-05, 2.659667419418924e-05, 3.153402559080532e-05, 3.153402559080532e-05, 3.153402559080532e-05, 4.4203865259639156e-05, 1.98991302647242e-05, 2.2988830491322855e-05, 4.4203865259639156e-05, 2.2988830491322855e-05, 1.690008785594207e-05, 1.6825059597066972e-05, 1.98991302647242e-05, 3.153402559080532e-05, 3.153402559080532e-05, 3.153402559080532e-05, 5.114117496660924e-05, 3.153402559080532e-05, 3.153402559080532e-05, 4.620843116421367e-05, 3.648294353148774e-05, 2.2988830491322855e-05, 3.153402559080532e-05, 3.222535488178904e-05, 2.3079114881507153e-05, 1.98991302647242e-05, 2.368930510631036e-05, 2.729585061925754e-05, 2.2988830491322855e-05, 2.3182031949702262e-05, 3.648294353148774e-05, 2.3182031949702262e-05, 2.3182031949702262e-05, 4.4203865259639156e-05, 2.729585061925754e-05, 3.222535488178904e-05, 1.6825059597066972e-05, 1.98991302647242e-05, 2.729585061925754e-05, 3.153402559080532e-05, 1.6825059597066972e-05, 4.438399499535952e-05, 2.729585061925754e-05, 1.6825059597066972e-05, 2.3182031949702262e-05, 1.690008785594207e-05, 3.153402559080532e-05, 2.729585061925754e-05, 5.346033540780142e-05, 5.114117496660924e-05, 2.3276498197484764e-05, 2.659667419418924e-05, 1.955237047518901e-05, 5.346033540780142e-05, 2.6820196493172625e-05, 3.153402559080532e-05, 2.8036444238215218e-05, 1.8344612287159934e-05, 2.3079114881507153e-05, 1.9144352373923113e-05, 2.729585061925754e-05, 3.153402559080532e-05, 1.98991302647242e-05, 2.2988830491322855e-05, 3.157963368548363e-05, 2.729585061925754e-05, 2.6820196493172625e-05, 3.153402559080532e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9611795656824086e-05, 1.8079724228813243e-05, 2.2656881983652844e-05, 2.6901710652804438e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 2.6901710652804438e-05, 3.5956146021485755e-05, 2.8121654918596474e-05, 1.658211320504353e-05, 1.6656058089165132e-05, 2.2847293698734397e-05, 1.9270042924724888e-05, 3.1078688259067336e-05, 2.2847293698734397e-05, 3.2488050957942216e-05, 1.658211320504353e-05, 3.112363779310369e-05, 4.48611306943613e-05, 3.1078688259067336e-05, 1.9611795656824086e-05, 3.112363779310369e-05, 3.8933720266003444e-05, 2.6901710652804438e-05, 2.2847293698734397e-05, 2.2847293698734397e-05, 2.2847293698734397e-05, 3.1078688259067336e-05, 3.5956146021485755e-05, 2.6901710652804438e-05, 3.5956146021485755e-05, 1.9270042924724888e-05, 2.621263002495327e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 4.48611306943613e-05, 1.9611795656824086e-05, 2.2656881983652844e-05, 4.48611306943613e-05, 2.2656881983652844e-05, 1.6656058089165132e-05, 1.658211320504353e-05, 1.9611795656824086e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 5.190159097093818e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 4.689550240602894e-05, 3.5956146021485755e-05, 2.2656881983652844e-05, 3.1078688259067336e-05, 3.270451234372244e-05, 2.274586270731966e-05, 1.9611795656824086e-05, 2.334724205613704e-05, 2.6901710652804438e-05, 2.2656881983652844e-05, 2.2847293698734397e-05, 3.5956146021485755e-05, 2.2847293698734397e-05, 2.2847293698734397e-05, 4.48611306943613e-05, 2.6901710652804438e-05, 3.270451234372244e-05, 1.658211320504353e-05, 1.9611795656824086e-05, 2.6901710652804438e-05, 3.1078688259067336e-05, 1.658211320504353e-05, 4.5043938771633e-05, 2.6901710652804438e-05, 1.658211320504353e-05, 2.2847293698734397e-05, 1.6656058089165132e-05, 3.1078688259067336e-05, 2.6901710652804438e-05, 5.425523491230885e-05, 5.190159097093818e-05, 2.2940395895832004e-05, 2.621263002495327e-05, 1.9270042924724888e-05, 5.425523491230885e-05, 2.6432924761159742e-05, 3.1078688259067336e-05, 2.7631610428650095e-05, 1.8079724228813243e-05, 2.274586270731966e-05, 1.886791642372411e-05, 2.6901710652804438e-05, 3.1078688259067336e-05, 1.9611795656824086e-05, 2.2656881983652844e-05, 3.112363779310369e-05, 2.6901710652804438e-05, 2.6432924761159742e-05, 3.1078688259067336e-05, ... ]\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0004314935168852e-05, 1.773179571047259e-05, 2.3110346986112803e-05, 2.7440133560958915e-05, 3.170071107129801e-05, 3.170071107129801e-05, 3.170071107129801e-05, 2.7440133560958915e-05, 3.667578846195254e-05, 2.868449433869145e-05, 1.691399505934024e-05, 1.698941990955269e-05, 2.3304569686700224e-05, 1.9655722210540504e-05, 3.170071107129801e-05, 2.3304569686700224e-05, 3.313828138762764e-05, 1.691399505934024e-05, 3.174656024547801e-05, 4.5759001493854045e-05, 3.170071107129801e-05, 2.0004314935168852e-05, 3.174656024547801e-05, 3.9712957213476494e-05, 2.7440133560958915e-05, 2.3304569686700224e-05, 2.3304569686700224e-05, 2.3304569686700224e-05, 3.170071107129801e-05, 3.667578846195254e-05, 2.7440133560958915e-05, 3.667578846195254e-05, 1.9655722210540504e-05, 2.673726136422245e-05, 3.170071107129801e-05, 3.170071107129801e-05, 3.170071107129801e-05, 4.5759001493854045e-05, 2.0004314935168852e-05, 2.3110346986112803e-05, 4.5759001493854045e-05, 2.3110346986112803e-05, 1.698941990955269e-05, 1.691399505934024e-05, 2.0004314935168852e-05, 3.170071107129801e-05, 3.170071107129801e-05, 3.170071107129801e-05, 5.294037270155289e-05, 3.170071107129801e-05, 3.170071107129801e-05, 4.7834090033808184e-05, 3.667578846195254e-05, 2.3110346986112803e-05, 3.170071107129801e-05, 3.335907512871192e-05, 2.32011086099099e-05, 2.0004314935168852e-05, 2.381452423486129e-05, 2.7440133560958915e-05, 2.3110346986112803e-05, 2.3304569686700224e-05, 3.667578846195254e-05, 2.3304569686700224e-05, 2.3304569686700224e-05, 4.5759001493854045e-05, 2.7440133560958915e-05, 3.335907512871192e-05, 1.691399505934024e-05, 2.0004314935168852e-05, 2.7440133560958915e-05, 3.170071107129801e-05, 1.691399505934024e-05, 4.594546837401264e-05, 2.7440133560958915e-05, 1.691399505934024e-05, 2.3304569686700224e-05, 1.698941990955269e-05, 3.170071107129801e-05, 2.7440133560958915e-05, 5.5341123528876965e-05, 5.294037270155289e-05, 2.3399535272946715e-05, 2.673726136422245e-05, 1.9655722210540504e-05, 5.5341123528876965e-05, 2.6961965178128482e-05, 3.170071107129801e-05, 2.818464187843392e-05, 1.773179571047259e-05, 2.32011086099099e-05, 1.9245547369309275e-05, 2.7440133560958915e-05, 3.170071107129801e-05, 2.0004314935168852e-05, 2.3110346986112803e-05, 3.174656024547801e-05, 2.7440133560958915e-05, 2.6961965178128482e-05, 3.170071107129801e-05, ... ]\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0431354479926306e-05, 1.8110322942845843e-05, 2.3603692151299445e-05, 2.6878344281779483e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 2.6878344281779483e-05, 3.5924915120989915e-05, 2.8097228924608794e-05, 1.7275064397309527e-05, 1.7352099369828212e-05, 2.2827448925724574e-05, 2.0075320216864204e-05, 3.1051693835916975e-05, 2.2827448925724574e-05, 3.24598323861808e-05, 1.7275064397309527e-05, 3.109660432754806e-05, 4.4822165074742076e-05, 3.1051693835916975e-05, 2.0431354479926306e-05, 3.109660432754806e-05, 3.889990309486298e-05, 2.6878344281779483e-05, 2.2827448925724574e-05, 2.2827448925724574e-05, 2.2827448925724574e-05, 3.1051693835916975e-05, 3.5924915120989915e-05, 2.6878344281779483e-05, 3.5924915120989915e-05, 2.0075320216864204e-05, 2.730803161844222e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 4.4822165074742076e-05, 2.0431354479926306e-05, 2.3603692151299445e-05, 4.4822165074742076e-05, 2.3603692151299445e-05, 1.7352099369828212e-05, 1.7275064397309527e-05, 2.0431354479926306e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 5.185651012656123e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 4.6854769765537664e-05, 3.5924915120989915e-05, 2.3603692151299445e-05, 3.1051693835916975e-05, 3.407120370210536e-05, 2.2726106035554007e-05, 2.0431354479926306e-05, 2.3326963036436993e-05, 2.6878344281779483e-05, 2.3603692151299445e-05, 2.2827448925724574e-05, 3.5924915120989915e-05, 2.2827448925724574e-05, 2.2827448925724574e-05, 4.4822165074742076e-05, 2.6878344281779483e-05, 3.407120370210536e-05, 1.7275064397309527e-05, 2.0431354479926306e-05, 2.6878344281779483e-05, 3.1051693835916975e-05, 1.7275064397309527e-05, 4.500481436800874e-05, 2.6878344281779483e-05, 1.7275064397309527e-05, 2.2827448925724574e-05, 1.7352099369828212e-05, 3.1051693835916975e-05, 2.6878344281779483e-05, 5.420810973259932e-05, 5.185651012656123e-05, 2.2920470255827934e-05, 2.730803161844222e-05, 2.0075320216864204e-05, 5.420810973259932e-05, 2.6409965569634143e-05, 3.1051693835916975e-05, 2.7607610079020822e-05, 1.8110322942845843e-05, 2.2726106035554007e-05, 1.9656389220871458e-05, 2.6878344281779483e-05, 3.1051693835916975e-05, 2.0431354479926306e-05, 2.3603692151299445e-05, 3.109660432754806e-05, 2.6878344281779483e-05, 2.6409965569634143e-05, 3.1051693835916975e-05, ... ]\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.014165627880483e-05, 1.785353487803956e-05, 2.3948139138803386e-05, 2.649723357298112e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 2.649723357298112e-05, 3.644916483202839e-05, 2.76988355295965e-05, 1.703011954624372e-05, 1.710606223224836e-05, 2.2503776263862036e-05, 1.979067026086265e-05, 3.1504828978085296e-05, 2.2503776263862036e-05, 3.293351639326896e-05, 1.703011954624372e-05, 3.0655682491282724e-05, 4.41866271516308e-05, 3.1504828978085296e-05, 2.014165627880483e-05, 3.0655682491282724e-05, 3.834833751160938e-05, 2.649723357298112e-05, 2.2503776263862036e-05, 2.2503776263862036e-05, 2.2503776263862036e-05, 3.1504828978085296e-05, 3.644916483202839e-05, 2.649723357298112e-05, 3.644916483202839e-05, 1.979067026086265e-05, 2.7706535766240003e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 4.41866271516308e-05, 2.014165627880483e-05, 2.3948139138803386e-05, 4.41866271516308e-05, 2.3948139138803386e-05, 1.710606223224836e-05, 1.703011954624372e-05, 2.014165627880483e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 5.11212315274423e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 4.619041133896475e-05, 3.644916483202839e-05, 2.3948139138803386e-05, 3.1504828978085296e-05, 3.358810472635018e-05, 2.240387032458025e-05, 2.014165627880483e-05, 2.2996207714467396e-05, 2.649723357298112e-05, 2.3948139138803386e-05, 2.2503776263862036e-05, 3.644916483202839e-05, 2.2503776263862036e-05, 2.2503776263862036e-05, 4.41866271516308e-05, 2.649723357298112e-05, 3.358810472635018e-05, 1.703011954624372e-05, 2.014165627880483e-05, 2.649723357298112e-05, 3.1504828978085296e-05, 1.703011954624372e-05, 4.436668664245693e-05, 2.649723357298112e-05, 1.703011954624372e-05, 2.2503776263862036e-05, 1.710606223224836e-05, 3.1504828978085296e-05, 2.649723357298112e-05, 5.3439487569484354e-05, 5.11212315274423e-05, 2.259547863530197e-05, 2.7706535766240003e-05, 1.979067026086265e-05, 5.3439487569484354e-05, 2.603549604903921e-05, 3.1504828978085296e-05, 2.7216159038170184e-05, 1.785353487803956e-05, 2.240387032458025e-05, 1.937767932900282e-05, 2.649723357298112e-05, 3.1504828978085296e-05, 2.014165627880483e-05, 2.3948139138803386e-05, 3.0655682491282724e-05, 2.649723357298112e-05, 2.603549604903921e-05, 3.1504828978085296e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0500122930295267e-05, 1.817127919739493e-05, 2.4374350823070265e-05, 2.6968812198948528e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 2.6968812198948528e-05, 3.709786074293565e-05, 2.8191799399351705e-05, 1.733320931422114e-05, 1.7410503573303005e-05, 2.290428222084659e-05, 2.014289031669928e-05, 3.2065529170424356e-05, 2.290428222084659e-05, 3.351964333237891e-05, 1.733320931422114e-05, 3.120127018773091e-05, 4.497302882865523e-05, 3.2065529170424356e-05, 2.0500122930295267e-05, 3.120127018773091e-05, 3.7689299656515175e-05, 2.6968812198948528e-05, 2.290428222084659e-05, 2.290428222084659e-05, 2.290428222084659e-05, 3.2065529170424356e-05, 3.709786074293565e-05, 2.6968812198948528e-05, 3.709786074293565e-05, 2.014289031669928e-05, 2.8199636679245626e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 4.497302882865523e-05, 2.0500122930295267e-05, 2.4374350823070265e-05, 4.497302882865523e-05, 2.4374350823070265e-05, 1.7410503573303005e-05, 1.733320931422114e-05, 2.0500122930295267e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 5.203105028475043e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 4.701247491975718e-05, 3.709786074293565e-05, 2.4374350823070265e-05, 3.2065529170424356e-05, 3.4185881555847315e-05, 2.2802598227812786e-05, 2.0500122930295267e-05, 2.2601004887151735e-05, 2.6968812198948528e-05, 2.4374350823070265e-05, 2.290428222084659e-05, 3.709786074293565e-05, 2.290428222084659e-05, 2.290428222084659e-05, 4.497302882865523e-05, 2.6968812198948528e-05, 3.4185881555847315e-05, 1.733320931422114e-05, 2.0500122930295267e-05, 2.6968812198948528e-05, 3.2065529170424356e-05, 1.733320931422114e-05, 4.3604220055903354e-05, 2.6968812198948528e-05, 1.733320931422114e-05, 2.290428222084659e-05, 1.7410503573303005e-05, 3.2065529170424356e-05, 2.6968812198948528e-05, 5.439056497350878e-05, 5.203105028475043e-05, 2.220716256370882e-05, 2.8199636679245626e-05, 2.014289031669928e-05, 5.439056497350878e-05, 2.649885700403737e-05, 3.2065529170424356e-05, 2.7700532580335656e-05, 1.817127919739493e-05, 2.2802598227812786e-05, 1.9722549270509703e-05, 2.6968812198948528e-05, 3.2065529170424356e-05, 2.0500122930295267e-05, 2.4374350823070265e-05, 3.120127018773091e-05, 2.6968812198948528e-05, 2.649885700403737e-05, 3.2065529170424356e-05, ... ]\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0151106561790354e-05, 1.7861911595155275e-05, 2.3959375389124898e-05, 2.650966583535975e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 2.650966583535975e-05, 3.64662664505587e-05, 2.771183157274712e-05, 1.703810992433188e-05, 1.7717367680664527e-05, 2.3307975433329763e-05, 2.0497913365325688e-05, 3.1519610758944444e-05, 2.3307975433329763e-05, 3.294896850134372e-05, 1.703810992433188e-05, 3.0670065860289074e-05, 4.42073591175118e-05, 3.1519610758944444e-05, 2.0151106561790354e-05, 3.0670065860289074e-05, 3.70476360654071e-05, 2.650966583535975e-05, 2.3307975433329763e-05, 2.3307975433329763e-05, 2.3307975433329763e-05, 3.1519610758944444e-05, 3.64662664505587e-05, 2.650966583535975e-05, 3.64662664505587e-05, 2.0497913365325688e-05, 2.771953542227202e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 4.42073591175118e-05, 2.0151106561790354e-05, 2.3959375389124898e-05, 4.42073591175118e-05, 2.3959375389124898e-05, 1.7717367680664527e-05, 1.703810992433188e-05, 2.0151106561790354e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 5.114521714698697e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 4.621208346226627e-05, 3.64662664505587e-05, 2.3959375389124898e-05, 3.1519610758944444e-05, 3.3603863961351405e-05, 2.2414382017938912e-05, 2.0151106561790354e-05, 2.2216220821363608e-05, 2.650966583535975e-05, 2.3959375389124898e-05, 2.3307975433329763e-05, 3.64662664505587e-05, 2.3307975433329763e-05, 2.3307975433329763e-05, 4.42073591175118e-05, 2.650966583535975e-05, 3.3603863961351405e-05, 1.703810992433188e-05, 2.0151106561790354e-05, 2.650966583535975e-05, 3.1519610758944444e-05, 1.703810992433188e-05, 4.2861854432675296e-05, 2.650966583535975e-05, 1.703810992433188e-05, 2.3307975433329763e-05, 1.7717367680664527e-05, 3.1519610758944444e-05, 2.650966583535975e-05, 5.346456089380002e-05, 5.114521714698697e-05, 2.259856888279968e-05, 2.771953542227202e-05, 2.0497913365325688e-05, 5.346456089380002e-05, 2.696590541917416e-05, 3.1519610758944444e-05, 2.81887608023348e-05, 1.7861911595155275e-05, 2.2414382017938912e-05, 1.9386771160912126e-05, 2.650966583535975e-05, 3.1519610758944444e-05, 2.0151106561790354e-05, 2.3959375389124898e-05, 3.0670065860289074e-05, 2.650966583535975e-05, 2.696590541917416e-05, 3.1519610758944444e-05, ... ]\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0382259818772148e-05, 1.7661612933992744e-05, 2.4234213281498865e-05, 2.6813758098509294e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 2.6813758098509294e-05, 3.688457083668089e-05, 2.8029713873916505e-05, 1.7233554010230912e-05, 1.792060352937445e-05, 2.3575341119603376e-05, 2.073304484166406e-05, 3.1881172078834616e-05, 2.3575341119603376e-05, 3.332692597776925e-05, 1.7233554010230912e-05, 3.10218820542888e-05, 4.471446154443028e-05, 3.1881172078834616e-05, 2.0382259818772148e-05, 3.10218820542888e-05, 3.7472609339888866e-05, 2.6813758098509294e-05, 2.3575341119603376e-05, 2.3575341119603376e-05, 2.3575341119603376e-05, 3.1881172078834616e-05, 3.688457083668089e-05, 2.6813758098509294e-05, 3.688457083668089e-05, 2.073304484166406e-05, 2.803750609426628e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 4.471446154443028e-05, 2.0382259818772148e-05, 2.4234213281498865e-05, 4.471446154443028e-05, 2.4234213281498865e-05, 1.792060352937445e-05, 1.7233554010230912e-05, 2.0382259818772148e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 5.173190371361871e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 4.67421820735491e-05, 3.688457083668089e-05, 2.4234213281498865e-05, 3.1881172078834616e-05, 3.398933374079112e-05, 2.2671497297975444e-05, 2.0382259818772148e-05, 2.2471062995163748e-05, 2.6813758098509294e-05, 2.4234213281498865e-05, 2.3575341119603376e-05, 3.688457083668089e-05, 2.3575341119603376e-05, 2.3575341119603376e-05, 4.471446154443028e-05, 2.6813758098509294e-05, 3.398933374079112e-05, 1.7233554010230912e-05, 2.0382259818772148e-05, 2.6813758098509294e-05, 3.1881172078834616e-05, 1.7233554010230912e-05, 4.335352258112224e-05, 2.6813758098509294e-05, 1.7233554010230912e-05, 2.3575341119603376e-05, 1.792060352937445e-05, 3.1881172078834616e-05, 2.6813758098509294e-05, 5.407785264260835e-05, 5.173190371361871e-05, 2.2857796969573413e-05, 2.803750609426628e-05, 2.073304484166406e-05, 5.407785264260835e-05, 2.7275231204633718e-05, 3.1881172078834616e-05, 2.8512113956652173e-05, 1.7661612933992744e-05, 2.2671497297975444e-05, 1.960915672978718e-05, 2.6813758098509294e-05, 3.1881172078834616e-05, 2.0382259818772148e-05, 2.4234213281498865e-05, 3.10218820542888e-05, 2.6813758098509294e-05, 2.7275231204633718e-05, 3.1881172078834616e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0023555222316973e-05, 1.735078862910382e-05, 2.3807718684096942e-05, 2.634186644548886e-05, 3.132009970183922e-05, 3.132009970183922e-05, 3.132009970183922e-05, 2.634186644548886e-05, 3.623544433083543e-05, 2.7536422781893576e-05, 1.6930263055660956e-05, 1.7605221284501043e-05, 2.3160441923057378e-05, 2.036816682767826e-05, 3.132009970183922e-05, 2.3160441923057378e-05, 3.27404099760972e-05, 1.6930263055660956e-05, 3.0475932204639896e-05, 4.553009361757194e-05, 3.132009970183922e-05, 2.0023555222316973e-05, 3.0475932204639896e-05, 3.815614350280283e-05, 2.634186644548886e-05, 2.3160441923057378e-05, 2.3160441923057378e-05, 2.3160441923057378e-05, 3.132009970183922e-05, 3.623544433083543e-05, 2.634186644548886e-05, 3.623544433083543e-05, 2.036816682767826e-05, 2.754407786802561e-05, 3.132009970183922e-05, 3.132009970183922e-05, 3.132009970183922e-05, 4.553009361757194e-05, 2.0023555222316973e-05, 2.3807718684096942e-05, 4.553009361757194e-05, 2.3807718684096942e-05, 1.7605221284501043e-05, 1.6930263055660956e-05, 2.0023555222316973e-05, 3.132009970183922e-05, 3.132009970183922e-05, 3.132009970183922e-05, 5.267554025571545e-05, 3.132009970183922e-05, 3.132009970183922e-05, 4.759480159642833e-05, 3.623544433083543e-05, 2.3807718684096942e-05, 3.132009970183922e-05, 3.460932981781329e-05, 2.227250472494314e-05, 2.0023555222316973e-05, 2.2075597837950156e-05, 2.634186644548886e-05, 2.3807718684096942e-05, 2.3160441923057378e-05, 3.623544433083543e-05, 2.3160441923057378e-05, 2.3160441923057378e-05, 4.553009361757194e-05, 2.634186644548886e-05, 3.460932981781329e-05, 1.6930263055660956e-05, 2.0023555222316973e-05, 2.634186644548886e-05, 3.132009970183922e-05, 1.6930263055660956e-05, 4.4144329901158925e-05, 2.634186644548886e-05, 1.6930263055660956e-05, 2.3160441923057378e-05, 1.7605221284501043e-05, 3.132009970183922e-05, 2.634186644548886e-05, 5.506428140722878e-05, 5.267554025571545e-05, 2.2455525734159485e-05, 2.754407786802561e-05, 2.036816682767826e-05, 5.506428140722878e-05, 2.679521814967949e-05, 3.132009970183922e-05, 2.8010333171702837e-05, 1.735078862910382e-05, 2.227250472494314e-05, 1.926405786861447e-05, 2.634186644548886e-05, 3.132009970183922e-05, 2.0023555222316973e-05, 2.3807718684096942e-05, 3.0475932204639896e-05, 2.634186644548886e-05, 2.679521814967949e-05, 3.132009970183922e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0289042804272142e-05, 1.7580838631064608e-05, 2.4123379594216702e-05, 2.669112702130331e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 2.669112702130331e-05, 3.6715881515424055e-05, 2.7901521697590238e-05, 1.715473740852227e-05, 1.7838644748851596e-05, 2.3467520743721182e-05, 2.0638223533388188e-05, 3.1735365494757385e-05, 2.3467520743721182e-05, 3.317450732695562e-05, 1.715473740852227e-05, 3.08800053804075e-05, 4.613376635832726e-05, 3.1735365494757385e-05, 2.0289042804272142e-05, 3.08800053804075e-05, 3.7663308975101216e-05, 2.669112702130331e-05, 2.3467520743721182e-05, 2.3467520743721182e-05, 2.3467520743721182e-05, 3.1735365494757385e-05, 3.6715881515424055e-05, 2.669112702130331e-05, 3.6715881515424055e-05, 2.0638223533388188e-05, 2.790927828069842e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 4.613376635832726e-05, 2.0289042804272142e-05, 2.4123379594216702e-05, 4.613376635832726e-05, 2.4123379594216702e-05, 1.7838644748851596e-05, 1.715473740852227e-05, 2.0289042804272142e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 5.337395278313144e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 4.822584981185157e-05, 3.6715881515424055e-05, 2.4123379594216702e-05, 3.1735365494757385e-05, 3.506820673474458e-05, 2.2567810596346033e-05, 2.0289042804272142e-05, 2.179046375899383e-05, 2.669112702130331e-05, 2.4123379594216702e-05, 2.3467520743721182e-05, 3.6715881515424055e-05, 2.3467520743721182e-05, 2.3467520743721182e-05, 4.613376635832726e-05, 2.669112702130331e-05, 3.506820673474458e-05, 1.715473740852227e-05, 2.0289042804272142e-05, 2.669112702130331e-05, 3.1735365494757385e-05, 1.715473740852227e-05, 4.3574150423352315e-05, 2.669112702130331e-05, 1.715473740852227e-05, 2.3467520743721182e-05, 1.7838644748851596e-05, 3.1735365494757385e-05, 2.669112702130331e-05, 5.5794365688496216e-05, 5.337395278313144e-05, 2.216548440913215e-05, 2.790927828069842e-05, 2.0638223533388188e-05, 5.5794365688496216e-05, 2.715048960849571e-05, 3.1735365494757385e-05, 2.838171555315056e-05, 1.7580838631064608e-05, 2.2567810596346033e-05, 1.9519475454822276e-05, 2.669112702130331e-05, 3.1735365494757385e-05, 2.0289042804272142e-05, 2.4123379594216702e-05, 3.08800053804075e-05, 2.669112702130331e-05, 2.715048960849571e-05, 3.1735365494757385e-05, ... ]\n",
      "Split on feature emp_length.2 years. (33652, 3572)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33652 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3572 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9997807148268495e-05, 1.7328477436837434e-05, 2.3777104595000504e-05, 2.6307993723073528e-05, 3.127982552288473e-05, 3.127982552288473e-05, 3.127982552288473e-05, 2.6307993723073528e-05, 3.618884956315053e-05, 2.750101399234814e-05, 1.690849261270128e-05, 1.758258292002279e-05, 2.313066016017814e-05, 2.0341975621277194e-05, 3.22043699581586e-05, 2.313066016017814e-05, 3.269830943545482e-05, 1.690849261270128e-05, 3.0436743531580138e-05, 4.5471546960453186e-05, 3.127982552288473e-05, 1.9997807148268495e-05, 3.0436743531580138e-05, 3.712267733454289e-05, 2.6307993723073528e-05, 2.313066016017814e-05, 2.313066016017814e-05, 2.313066016017814e-05, 3.127982552288473e-05, 3.618884956315053e-05, 2.6307993723073528e-05, 3.618884956315053e-05, 2.0341975621277194e-05, 2.7508659234887343e-05, 3.22043699581586e-05, 3.22043699581586e-05, 3.127982552288473e-05, 4.5471546960453186e-05, 1.9997807148268495e-05, 2.3777104595000504e-05, 4.5471546960453186e-05, 2.3777104595000504e-05, 1.758258292002279e-05, 1.690849261270128e-05, 1.9997807148268495e-05, 3.22043699581586e-05, 3.127982552288473e-05, 3.127982552288473e-05, 5.2607805345706253e-05, 3.127982552288473e-05, 3.22043699581586e-05, 4.7533599953552164e-05, 3.618884956315053e-05, 2.3777104595000504e-05, 3.127982552288473e-05, 3.456482605327082e-05, 2.2243864750945727e-05, 1.9997807148268495e-05, 2.147767620816181e-05, 2.6307993723073528e-05, 2.3777104595000504e-05, 2.313066016017814e-05, 3.618884956315053e-05, 2.313066016017814e-05, 2.313066016017814e-05, 4.5471546960453186e-05, 2.6307993723073528e-05, 3.456482605327082e-05, 1.690849261270128e-05, 1.9997807148268495e-05, 2.6307993723073528e-05, 3.127982552288473e-05, 1.690849261270128e-05, 4.2948672602354546e-05, 2.6307993723073528e-05, 1.690849261270128e-05, 2.313066016017814e-05, 1.758258292002279e-05, 3.127982552288473e-05, 2.7085584671657706e-05, 5.49934748406946e-05, 5.2607805345706253e-05, 2.1847313687388046e-05, 2.7508659234887343e-05, 2.0341975621277194e-05, 5.49934748406946e-05, 2.6760762467189393e-05, 3.127982552288473e-05, 2.797431498589017e-05, 1.7328477436837434e-05, 2.2901331377779227e-05, 1.9239286424034918e-05, 2.6307993723073528e-05, 3.127982552288473e-05, 1.9997807148268495e-05, 2.3777104595000504e-05, 3.0436743531580138e-05, 2.6307993723073528e-05, 2.755173599754443e-05, 3.127982552288473e-05, ... ]\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0234593284607894e-05, 1.7128044279078494e-05, 2.405863989977893e-05, 2.661949628644669e-05, 3.165019758298211e-05, 3.165019758298211e-05, 3.165019758298211e-05, 2.661949628644669e-05, 3.6617347438097185e-05, 2.7826642637548256e-05, 1.7108699395745064e-05, 1.7790771340164633e-05, 2.340454117171607e-05, 2.058283692057668e-05, 3.2585689183766656e-05, 2.340454117171607e-05, 3.30854772033329e-05, 1.7108699395745064e-05, 3.079713299079243e-05, 4.6009957589094214e-05, 3.165019758298211e-05, 2.0234593284607894e-05, 3.079713299079243e-05, 3.756223229531676e-05, 2.661949628644669e-05, 2.340454117171607e-05, 2.340454117171607e-05, 2.340454117171607e-05, 3.165019758298211e-05, 3.6617347438097185e-05, 2.661949628644669e-05, 3.6617347438097185e-05, 2.058283692057668e-05, 2.7834378404384888e-05, 3.2585689183766656e-05, 3.2585689183766656e-05, 3.165019758298211e-05, 4.6009957589094214e-05, 2.0234593284607894e-05, 2.405863989977893e-05, 4.6009957589094214e-05, 2.405863989977893e-05, 1.7790771340164633e-05, 1.7108699395745064e-05, 2.0234593284607894e-05, 3.2585689183766656e-05, 3.165019758298211e-05, 3.165019758298211e-05, 5.323071359143276e-05, 3.165019758298211e-05, 3.2585689183766656e-05, 4.809642653727981e-05, 3.6617347438097185e-05, 2.405863989977893e-05, 3.165019758298211e-05, 3.497409450724259e-05, 2.250724556828142e-05, 2.0234593284607894e-05, 2.173198488956739e-05, 2.661949628644669e-05, 2.405863989977893e-05, 2.340454117171607e-05, 3.6617347438097185e-05, 2.340454117171607e-05, 2.340454117171607e-05, 4.6009957589094214e-05, 2.661949628644669e-05, 3.497409450724259e-05, 1.7108699395745064e-05, 2.0234593284607894e-05, 2.661949628644669e-05, 3.165019758298211e-05, 1.7108699395745064e-05, 4.3457210872126687e-05, 2.661949628644669e-05, 1.7108699395745064e-05, 2.340454117171607e-05, 1.7790771340164633e-05, 3.165019758298211e-05, 2.7406294382344698e-05, 5.564463085669476e-05, 5.323071359143276e-05, 2.210599910019739e-05, 2.7834378404384888e-05, 2.058283692057668e-05, 5.564463085669476e-05, 2.7077626086441306e-05, 3.165019758298211e-05, 2.8305547801225315e-05, 1.7128044279078494e-05, 2.3172496997777823e-05, 1.9467091216055263e-05, 2.661949628644669e-05, 3.165019758298211e-05, 2.0234593284607894e-05, 2.405863989977893e-05, 3.079713299079243e-05, 2.661949628644669e-05, 2.7877965221975494e-05, 3.165019758298211e-05, ... ]\n",
      "Split on feature home_ownership.OWN. (34149, 3075)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34149 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3075 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9962491863571e-05, 1.7364736649484144e-05, 2.373511523027522e-05, 2.6261534914802107e-05, 3.1224586669172496e-05, 3.1224586669172496e-05, 3.1224586669172496e-05, 2.6261534914802107e-05, 3.612494157985406e-05, 2.7452448360556172e-05, 1.6878632927286987e-05, 1.7551532819532563e-05, 2.3089812389082975e-05, 2.0306052549559947e-05, 3.214749839793484e-05, 2.3089812389082975e-05, 3.264056559892923e-05, 1.6878632927286987e-05, 3.0382993525136288e-05, 4.539124612473466e-05, 3.1224586669172496e-05, 1.9962491863571e-05, 3.0382993525136288e-05, 3.808130462224704e-05, 2.6261534914802107e-05, 2.3089812389082975e-05, 2.3089812389082975e-05, 2.3089812389082975e-05, 3.1224586669172496e-05, 3.612494157985406e-05, 2.6261534914802107e-05, 3.612494157985406e-05, 2.0306052549559947e-05, 2.746008010191923e-05, 3.214749839793484e-05, 3.214749839793484e-05, 3.1224586669172496e-05, 4.539124612473466e-05, 1.9962491863571e-05, 2.373511523027522e-05, 4.66457689956147e-05, 2.373511523027522e-05, 1.8036621932984037e-05, 1.6878632927286987e-05, 2.0514215042392473e-05, 3.214749839793484e-05, 3.1224586669172496e-05, 3.1224586669172496e-05, 5.2514902178407995e-05, 3.1224586669172496e-05, 3.214749839793484e-05, 4.7449657619144665e-05, 3.612494157985406e-05, 2.4391106141331697e-05, 3.1224586669172496e-05, 3.450378602705493e-05, 2.2204583023172878e-05, 1.9962491863571e-05, 2.143974753706761e-05, 2.6261534914802107e-05, 2.373511523027522e-05, 2.3089812389082975e-05, 3.612494157985406e-05, 2.3089812389082975e-05, 2.3089812389082975e-05, 4.539124612473466e-05, 2.6987350991419605e-05, 3.450378602705493e-05, 1.7345124439290927e-05, 1.9962491863571e-05, 2.6261534914802107e-05, 3.1224586669172496e-05, 1.6878632927286987e-05, 4.287282705643613e-05, 2.6261534914802107e-05, 1.6878632927286987e-05, 2.3089812389082975e-05, 1.7551532819532563e-05, 3.1224586669172496e-05, 2.70377526705395e-05, 5.4896358681602544e-05, 5.2514902178407995e-05, 2.1808732252082407e-05, 2.746008010191923e-05, 2.0306052549559947e-05, 5.4896358681602544e-05, 2.671350408839596e-05, 3.2087571527587897e-05, 2.7924913524488957e-05, 1.689771767343007e-05, 2.286088859164965e-05, 1.9205310654971643e-05, 2.6261534914802107e-05, 3.1224586669172496e-05, 1.9962491863571e-05, 2.373511523027522e-05, 3.0382993525136288e-05, 2.6261534914802107e-05, 2.7503080792828757e-05, 3.1224586669172496e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.019462171711352e-05, 1.7566659024846763e-05, 2.4011114281890267e-05, 2.6566911933625063e-05, 3.158767554504915e-05, 3.158767554504915e-05, 3.158767554504915e-05, 2.6566911933625063e-05, 3.65450132550474e-05, 2.7771673678761494e-05, 1.7074902742507754e-05, 1.7755627316887366e-05, 2.3358307665365748e-05, 2.0542177429989223e-05, 3.2521319168701894e-05, 2.3358307665365748e-05, 3.302011990326191e-05, 1.7074902742507754e-05, 3.0736296103059625e-05, 4.591906917343307e-05, 3.158767554504915e-05, 2.019462171711352e-05, 3.0736296103059625e-05, 3.7648548212870725e-05, 2.6566911933625063e-05, 2.3358307665365748e-05, 2.3358307665365748e-05, 2.3358307665365748e-05, 3.158767554504915e-05, 3.65450132550474e-05, 2.6566911933625063e-05, 3.65450132550474e-05, 2.0542177429989223e-05, 2.7779394164306235e-05, 3.2521319168701894e-05, 3.2521319168701894e-05, 3.158767554504915e-05, 4.591906917343307e-05, 2.019462171711352e-05, 2.4011114281890267e-05, 4.718818001320363e-05, 2.4011114281890267e-05, 1.824635719230534e-05, 1.7074902742507754e-05, 2.075276049883523e-05, 3.2521319168701894e-05, 3.158767554504915e-05, 3.158767554504915e-05, 5.3125561240151654e-05, 3.158767554504915e-05, 3.2521319168701894e-05, 4.8001416495193345e-05, 3.65450132550474e-05, 2.467473325236685e-05, 3.158767554504915e-05, 3.490500641836985e-05, 2.2462784586402963e-05, 2.019462171711352e-05, 2.1196105985022257e-05, 2.6566911933625063e-05, 2.4011114281890267e-05, 2.3358307665365748e-05, 3.65450132550474e-05, 2.3358307665365748e-05, 2.3358307665365748e-05, 4.591906917343307e-05, 2.730116801766826e-05, 3.490500641836985e-05, 1.7546818757980516e-05, 2.019462171711352e-05, 2.6566911933625063e-05, 3.158767554504915e-05, 1.7074902742507754e-05, 4.238561972777975e-05, 2.6566911933625063e-05, 1.7074902742507754e-05, 2.3358307665365748e-05, 1.7755627316887366e-05, 3.158767554504915e-05, 2.7352155782657212e-05, 5.553471003512433e-05, 5.3125561240151654e-05, 2.156089755324315e-05, 2.7779394164306235e-05, 2.0542177429989223e-05, 5.553471003512433e-05, 2.7024136740573164e-05, 3.246069545070026e-05, 2.824963280958264e-05, 1.7094209411813403e-05, 2.3126721873230874e-05, 1.9428635777909463e-05, 2.6566911933625063e-05, 3.158767554504915e-05, 2.019462171711352e-05, 2.4011114281890267e-05, 3.0736296103059625e-05, 2.6566911933625063e-05, 2.7822894880170128e-05, 3.158767554504915e-05, ... ]\n",
      "Split on feature grade.C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "# this may take a while... \n",
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
    "                                 features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing training error at the end of each iteration\n",
    "\n",
    "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.421636578551\n",
      "Iteration 2, training error = 0.433430045132\n",
      "Iteration 3, training error = 0.400037610144\n",
      "Iteration 4, training error = 0.400037610144\n",
      "Iteration 5, training error = 0.384724908661\n",
      "Iteration 6, training error = 0.384617451107\n",
      "Iteration 7, training error = 0.382763808296\n",
      "Iteration 8, training error = 0.384617451107\n",
      "Iteration 9, training error = 0.382763808296\n",
      "Iteration 10, training error = 0.384483129164\n",
      "Iteration 11, training error = 0.382736943907\n",
      "Iteration 12, training error = 0.381447453256\n",
      "Iteration 13, training error = 0.381528046422\n",
      "Iteration 14, training error = 0.380560928433\n",
      "Iteration 15, training error = 0.380507199656\n",
      "Iteration 16, training error = 0.378223726628\n",
      "Iteration 17, training error = 0.378277455405\n",
      "Iteration 18, training error = 0.378411777348\n",
      "Iteration 19, training error = 0.378062540297\n",
      "Iteration 20, training error = 0.378761014399\n",
      "Iteration 21, training error = 0.379566946056\n",
      "Iteration 22, training error = 0.378895336342\n",
      "Iteration 23, training error = 0.378895336342\n",
      "Iteration 24, training error = 0.378761014399\n",
      "Iteration 25, training error = 0.378895336342\n",
      "Iteration 26, training error = 0.378975929508\n",
      "Iteration 27, training error = 0.379110251451\n",
      "Iteration 28, training error = 0.378922200731\n",
      "Iteration 29, training error = 0.379029658285\n",
      "Iteration 30, training error = 0.378734150011\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = 1.0 - graphlab.evaluation.accuracy(train_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training error vs number of iterations\n",
    "\n",
    "We have provided you with a simple code snippet that plots classification error with the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcMAAAFNCAYAAAB8PAR2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wfi4Bj8hWvPUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f0b72b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Which of the following best describes a **general trend in accuracy** as we add more and more components? Answer based on the 30 components learned so far.\n",
    "\n",
    "1. Training error goes down monotonically, i.e. the training error reduces with each iteration but never increases.\n",
    "2. Training error goes down in general, with some ups and downs in the middle.\n",
    "3. Training error goes up in general, with some ups and downs in the middle.\n",
    "4. Training error goes down in the beginning, achieves the best error, and then goes up sharply.\n",
    "5. None of the above\n",
    "\n",
    "\n",
    "### Evaluation on the test data\n",
    "\n",
    "Performing well on the training data is cheating, so lets make sure it works on the `test_data` as well. Here, we will compute the classification error on the `test_data` at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, test error = 0.42330891857\n",
      "Iteration 2, test error = 0.428479103835\n",
      "Iteration 3, test error = 0.398104265403\n",
      "Iteration 4, test error = 0.398104265403\n",
      "Iteration 5, test error = 0.379900904782\n",
      "Iteration 6, test error = 0.380008616975\n",
      "Iteration 7, test error = 0.379254631624\n",
      "Iteration 8, test error = 0.380008616975\n",
      "Iteration 9, test error = 0.379254631624\n",
      "Iteration 10, test error = 0.379685480396\n",
      "Iteration 11, test error = 0.379254631624\n",
      "Iteration 12, test error = 0.377962085308\n",
      "Iteration 13, test error = 0.379254631624\n",
      "Iteration 14, test error = 0.377854373115\n",
      "Iteration 15, test error = 0.378500646273\n",
      "Iteration 16, test error = 0.377854373115\n",
      "Iteration 17, test error = 0.377962085308\n",
      "Iteration 18, test error = 0.377854373115\n",
      "Iteration 19, test error = 0.378177509694\n",
      "Iteration 20, test error = 0.376884963378\n",
      "Iteration 21, test error = 0.377531236536\n",
      "Iteration 22, test error = 0.376777251185\n",
      "Iteration 23, test error = 0.376777251185\n",
      "Iteration 24, test error = 0.376884963378\n",
      "Iteration 25, test error = 0.376777251185\n",
      "Iteration 26, test error = 0.376561826799\n",
      "Iteration 27, test error = 0.376454114606\n",
      "Iteration 28, test error = 0.376992675571\n",
      "Iteration 29, test error = 0.376777251185\n",
      "Iteration 30, test error = 0.376777251185\n"
     ]
    }
   ],
   "source": [
    "test_error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
    "    error = 1.0 - graphlab.evaluation.accuracy(test_data[target], predictions)\n",
    "    test_error_all.append(error)\n",
    "    print \"Iteration %s, test error = %s\" % (n, test_error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize both the training and test errors\n",
    "\n",
    "Now, let us plot the training & test error with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFTCAYAAAAKvWRNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XeYVNX5wPHvu33pdQGRDiKCjaBigg1U7AYbRqNYItZf\nYtQgKgpiD7bEXmI3sWKJGmIU0GhAxYKKKNJ73wUWFra9vz/Ond2Z2Tuzd9mdnd3h/TzPPLv33HPv\nPTNzZ94595Qrqooxxhhjkict2QUwxhhjdnUWjI0xxpgks2BsjDHGJJkFY2OMMSbJLBgbY4wxSWbB\n2BhjjEkyC8amChEZJSLfishWEVERuTLZZTLxicjxIvKZiGz23rP76/n4i0VkcbL3YRoP7zydXoP8\nz3jbdE9YoZLIgnEDIyLdvRMu/LHD+6J6SkR6Jfj4Q4BngGzgAeBmYGYij2lqxzsnJgNdgMdx79mU\nGmx/Yti5dnCCipmSRORw73WbkOyymMYtI9kFMDH9DPzd+78FcDhwPjBCRA5S1XkJOu6x3t9RqmpB\nuHEYCmQBV6nqSzux/QWAAuL9P6MOy2aMCcCCccM1T1UnhBZERICngVHADd7fROjk/V2doP2burfT\n75mI5AHHA//x9jNSRP6gqtvqsHzGmGrYZepGQt28pQ97i4PC14lIRxH5q4gs9C5prxGRF0SkR/R+\nQu00ItLFy7NGRMpF5EoRUVztG2BR6NJl1PYXicgsrz15i4j8V0RG+Bxngrf94SJygYh8LSJFIvKm\nz/oLRWSOt/4nETnHy5MtIneKyFIR2S4iX/hdRhWRoSLytIjMCyvX/0RkpE/eUDPAMyLSW0TeEJF8\nb7sPRGRfv9ffy/s3ryw7RGSViPxbRE6KypfmvUafiUih9/ifiJzit99YRCRPRB4UkSUiUuwd79nw\n9zT0XHCXpQGmhV1u7h7wUOcAmcALwPNAc+D0OOU6QkQ+FZFtIrLWe03axMi7h4hMEpFvvNd4u/c+\njxORzDjHaOPtd613TswUkWNj5O3hvS6rvNdpiYg8ICLtY+Qf4Z2zW7z3fJaI/M4nX5qIXOytz/ee\n71IReV1EfuHlmQBM8zYZL5FNS91jPb+wY2SLyJ9EZLa3/03eOXiYT97p3n4zvc/OIu88nCcil/nk\nzxGRMSLynfdcC8V9P7wgUU1dNTlnpbLdtqeIXCsi87336BsROcbL01JEHhWR1d66qSKyZ5zXoZuI\nvCoiG733ZKqIHFjd67cz5W/QVNUeDegBdMddMnzHZ92B3rrvw9L6ACuAMuCfwCTgJaAYWAf0itqH\nAt8By4CvgPuBJ4A9gAnAN16e+73lCWHbPuytWwzcCzwIrPXS/hR1nAle+r+AQuAfwJ3ADVHr3wI2\nAE8BDwHrvfTjveczD9d2/TxQChQAraKONcXL97x3jMdxtUQFrozx+k73jvURcA/wppe+EegQtc2h\nwBbvNX4buMN7zb4F3gzLJ8DL3n7meM/nIe/1UuAPAc+BPGCRt8373vEmA+Vemff08rXyXsfpXt5n\nQu9Z9GsU51jfe+9PU2A37zl+FCPv0UAJsA34G3AX7lz6ClgJLI7KP9Yr7yvA3d77+K1X1jd99r/Y\n289XXrnu8l7n0Gt/alT+ft7+y73X5w5cDV+BhUBeVP4x3rq1uHP3XmCJl/ZQVN5JXvps3GfhLuBF\nr3xXeHkO917z0Pk0gYCvP5ADfOxt+zmVn8O1uPP8lKj8off4NWAp8Bju8xj6vFwUlf9VL/0T73ne\n7aVtAE7Y2XM27Pm+BSwHHvHKvQ3YARwAfAl8DdwHvOHlXwCk+3wXzfaez0zv/Xse991VBPwyxrG7\n1/VnriE8kl4Ae0S9IfGD8VPeuqfD0mZ4H4JDo/IejPvifCcqXb3HE0CazzGqnPBe+uFhH55mYem7\nAau8Y/UKS5/g5d8M9Pc5Tmj9OqBbWPovvPR87wsoN2zd1d66a6JfM5/9N/XKuglo4vP6KnBt1Da3\neOljw9JycD92SoHDfY7TOez/i73tHwr/4vHK8pn3Pu0W4BwIvQc3RaWH2nanxXgtq5SvmuMc5G33\nfFja+7jgFv0jLh33A6EUOCAsPQP40NvP4qhtdgOyotLEO/cUGBK1brGX/iGQEZa+F7AdF6hywtKn\ne/nPjdrPRKp+Tnp7ZV8BdAxLb44L/AocFpa+EZhF1QCSBrT2+VxMqOFrf0f0uealt/deh3VEnvuh\n5zoTaBGW3hf32fsxLK2l9x6+4XPcLKD5zp6zYefmXKBtWPqpVH5u/xG1rwe8dadFlSX0OXw2Kn2Y\nl/5tjM9F950tf0N+JL0A9oh6QyqDxTwqf2Xf630xqPcl0cfLO9BLezjGvl7D1ShahqUp7outbYxt\nqpzwXvrTXvoIn22uISp4UBkg7o5xnND6G33WzffWHRKVvrvfhzfOa3kVUUEq7PVdSNSPkbB1r4el\njfTSngxwvG9xNfcsn3UnePu5opp9ZONqBWsICzzeOsHVRBXo6vNaHl5dGaP295i33fCwtHO8tNui\n8h7qpb/ms59f4hOM4xw3dN5OiEpf7KX/0mebx8PPP6Cbt/yNT95cXOAuCr0XwHgv/x998p/mrXsq\nLG0jrlYp1TyXw/2eSzXbpOGC1pwY66/w9hleg53upR3hkz+0rrm33MJb/ntdn7NUfj+c4/Ocdnjr\nukStG+Kl3xyVrrgfSF18jv2+t35/n2N3r8vPXEN5WAeuhqsP7gsE3C/flbia8a2qushLP8j721n8\nh1Z0wn1I+uCCechiVd1Qw/KE2lI/8lk3PSpPuFk+aeFm+6StAnr5rAt1UNotPFFEWuAuQZ4M9ASa\nRG3Xiaq+UdXyqLTl3t9WYWkHeH/f99lHeBmaAANwl/+vF5HoLKE2zJhtZ56+uNr4f1R1e/gKVVUR\n+cg7zr64y3s7RURygTNxr+kHYasm4y49nisiN4a9RqH39hOf3c3EfalGHyMNuBA4D+iPCxLhL4zf\n+1KC/1C6T4CLvHK8EVaej6MzqmqRiHyOa+roi/sBU9Pz92XgEuArEXnNy/OFqhb7bF9TfXHn2NIY\nn9s+3t89gXei1n3pkz/8vN2iqptFZArwGxHZHdcE8xHunC8LbVTLczbis6mq5SKyFmiqqsui8vp+\nbj1LfPKDe7+Pwr0nX/usr8vPXINgwbjheldVT6gmT6jjzEneI5amUctrdqI8LYBSVd3os251WJ5o\n1R1rs09aGYCqRqxT1VLvA1fR+UdEsnBfNPvhvqiewdVqyry0k3G1zWqPG7b/9LDklt7fldU8j9a4\nQNOVyh9RfqLfi2ih1zDW6xbvta6J07x9/C38C1pVt4rIG8BvgeG4Nn+ofB3WRe/I+yJe73OMB4DL\ncO2yk72yF+OCxh/wf182+PxIgsrXI1SOmr5OMfOr6noRKSXyNf097rL8+cCtXtoWEXkOd2m5MMZx\ngwh9bvfxHrFUOVeiPxOe0A+h8PP2NGAc8BtcnwiADSLyMHCLqpZQu3M21uc2Xvn8Ou2tjXHM6Pfb\nT1195hoEC8aNW+jEv1RVH63BdrqTx8oQkTY+AblDVHlqe6yaOBkXdJ9Q1dHhK0TkWm99bRR4f/1+\n1YcLPffPVHVwLY4X2k+HGOvjvdY1cYH3948i8sc4eULBeJP3t0ovZa8G3A7XHhtK6wBciqtBHayq\nRWHrDsIFYz9tRSTNJyCHnneoHDV9ncLzrwjPKCJtcd+FFa+pF6z+DPzZq10egauZX45rZx4V47hB\nhI7zsqqeWYv9xKSqW4HrgOtEpDduLPrlwI24z+R46u6crY28GOnR77efhlD+OmNDmxq3z72/9XEi\nfuP9PdRn3WFReepTaJjG2z7rflUH+//C+3t0vEyqugX4EdhLRJrX4ng/4dr0DxQRv5pj6PX3u7wf\niIj0xL1nK3G9ov0eG4CTvEAVfrwhPrscTNUf9j1wtZYPwgOxJ977kon/+Rw6bqgcoXPtkOiMIpKD\nG3mwHfd6huev8fmrqstV9XngSNyVgfCrUKGrCulVNoxtLq6H+CARqcl2O0VV56vq47gfFOV45a/D\nc7Y2uolIF5/06Pe7igZS/jpjwbgRU9XPcAH5HBH5dfR6b0yi35fnznjO+zteRCou+4hIR1wHrlIq\nZwyrT6F204gveG+M4Yl1sP+3cUHrPBE5PHqliHQOW3wAV2t61AsI0Xn7i5tkIyZV3YFrr+yA6z0e\nvv0o3GXN6aq60+3FuEuvAjyoqr/ze+D6J2ThLlcDfIrrYPVrEQm1oyMiGbhe6NFC5TtYwhrzRGQP\nXI0tnlu8/Ya22Qs4FxcI/wXgPf+PgP1E5Kyo7a/F1bheCmvj/TsucF4T/h6ISDNcBzjwznFv/O/Q\n8HJ7muM6h4W35YeuEvkFFF+qWgo8ivsheYdfQBaRg7w20RoTkfbh71GYPNx3fnj5a33O1lI6lc0A\noWMOw7UXf6+qvu3FYZJd/jpjl6kbv7NwEw+8ISKf4Do7lOJ6mx6C+7KodQcGVZ0uIo/gLj1+57Ur\nZgFn4D7kY1R1QW2PsxP+ifviv1ZE+uN+KfcHjsF19KkyIUlNqOp2EfkN8B7woYi8ixsK0wbXgW4J\nEPoh9AiuZ/HZwCEiMhXXdtkJ2BvYHzfkLFY7WcgYXG3tNhE5FDfutq/3XDbg3oOd4l1SHoWrIT0X\nJ+vTwJ9wgfsvqlomIpfgOhR9JCL/wI1xPQ7X6WpV+MaqutI7R0YAX4jINNyl/pOAf+OGwvhZhWsL\n/MZ7rVvj2j0zgcuiOrVdiuvo87z342sebmjc0bj23mvDyjNfRK7HjRf+VkRe9co9AteL/mFVDXXu\nysUNr1ogIp/hOgi19MoeHrzBnW8rgTNFZAeuM5UCD6hqvEusN+Em7/kT7grEf3Gf1d2959AXd97s\nzExonYHPReQ73PfBStxn9Nde2e4Ny1tX5+zO+hY4QkRm4DrJ7Y77TtmOG7ZUnWSXv+4kuzu3PSIf\nxBlnHGebtrhxiz/ghnNsxl0K+xswLCqv4mpWsfb1DD5Dm7x1gms3+xL3JVEI/JeoCQq8vBOIM9wm\n3nq8oRoxtqtSflwN4w1czWmLV6bhuF68Cpzn8/o+E3T/XnpfXPBaieuEtApXSzveJ+/ZuB9I+bjh\nHktxAehSXG/TIO9pHm5iiqXe8VZ7x+9R09c6Ku9wL++UAHlnenl/EZY2FPifd56tw9Wg2+BqzYuj\ntm+Om8xiCe7LdQ6uY1QPv/cgtA9vf3/DfYFux40XPTZGGXt6r0uoc9hS73XLi5H/FFwAL/TO4S+p\nOmFGJi6Qv48Lrju89/s/wIk++zzIO2c3Uzl2tsrnx2e7DFw77kxv2yLckLs3cVcCwsdaTyf2Z+KZ\n8GPiOsiNx105WOWVfxluoo4hMfYR6JyNPpbf++eT3j3G+63e8+qGG4a50XtPpgIHVfc86/ozl+yH\neE/EGGOMMUlibcbGGGNMklkwNsYYY5LMgrExxhiTZBaMjTHGmCSzoU0+2rVrp927d092MYwxxjRy\nX3755XpV9b2/djgLxj66d+/OrFnV3d/AGGOMiU9ElgTJZ5epjTHGmCSzYGyMMcYkmQVjY4wxJsks\nGBtjjDFJZsHYGGOMSTILxsYYY0yS2dAmY0xK2bx5M2vXrqWkpCTZRTEpLjMzk7y8PFq0aFHrfVkw\nNsakjM2bN7NmzRo6d+5Mbm4uIpLsIpkUpaoUFRWxYsUKgFoHZLtM3UAs27iNif/8gb9++DPbS8qS\nXRxjGqW1a9fSuXNnmjRpYoHYJJSI0KRJEzp37szatWtrvT+rGTcAJWXljHr6cxau2wrAivwi7jpt\nnySXypjGp6SkhNzc3GQXw+xCcnNz66RJxGrGDcDsZQUVgRhg2k+1/5VlzK7KasSmPtXV+WbBuAGY\nsWBDxPKGrcWUl2uSSmOMMaa+WTBuAP4XFYzLypX8bcVJKo0xxpj6ZsE4ybaXlPHl0vwq6esLLRgb\ns6sRkWof06dPr/VxOnbsyLhx42q0zfbt2xERnnzyyVof31RlHbiS7Kul+RSXlldJX1+4g740T0KJ\njDHJMmPGjIr/i4qKGDp0KOPGjeP444+vSN9rr71qfZz33nuPvLy8Gm2TnZ3NjBkz6NWrV62Pb6qy\nYJxkM6MuUYesL9xRzyUxxiTb4MGDK/4vLCwEoFevXhHpsWzfvp2cnJxAxxk4cGCNyyYigcqRbKpK\ncXEx2dnZVdYVFRXtdG/74uJiMjIySEtLzAVlu0ydZNHtxSHrtlgwNsb4e/TRRxERvvrqKw455BBy\nc3N54IEHUFWuvvpqBgwYQNOmTenSpQujRo1i3bp1EdtHX6Y+88wzGTJkCO+99x79+/enWbNmHHbY\nYfz0008VefwuUw8ePJjf/va3PPvss/Ts2ZMWLVpw4oknsnr16ojjLVy4kKOOOorc3Fx69erF3//+\nd0444QSOOeaYap/ra6+9xsCBA8nJyWG33XbjhhtuoKysci6GsWPHsvvuuzNt2jQGDhxIdnY2b7/9\nNlOmTEFEmDp1KscddxxNmzblmmuuAdwPncsuu4y8vDxycnI46KCDmDZtWsRxQ8/twQcfpEePHuTm\n5rJhg//3dV2wmnESbSsu5ZtlBb7r1lnN2Jha6z723WQXAYDFdx5ffaadMHLkSC6//HImTpxImzZt\nKC8vZ+PGjYwbN45OnTqxZs0aJk2axFFHHcXXX38ddxjO/PnzGTduHBMmTCAzM5OrrrqKs846iy+/\n/DJuGT7++GOWLl3K/fffz+bNm7nyyiu57LLLmDx5MgDl5eWccMIJFBcX88wzz5CRkcHNN9/Mxo0b\nGTBgQNx9P/fcc5x//vlcccUV3Hnnnfz0009cf/31iAi33nprRb5Nmzbxu9/9juuuu46ePXvStWtX\n5s+fD8B5553HhRdeyDXXXEOTJk0AGDVqFB988AF33nkn3bp145FHHmH48OF88sknHHjggRX7/fDD\nD5k3bx733HMPWVlZFdsnggXjJPpicT6lMYYwrd9iHbiMMfFdc801XHzxxRFpTz/9dMX/ZWVl/OIX\nv6B379588cUXEYEm2saNG/nss8/o1q0b4GrCv/nNb1i8eDHdu3ePud3WrVt59913ad7c9XFZvnw5\n48aNo7S0lIyMDN544w3mzp3L7Nmz2WcfN5nRwIED6d27d9xgXFZWxrXXXsvo0aP5y1/+AsDRRx9N\neno6Y8aMYcyYMRVTUBYWFvLaa68xfPjwiu1Dwfjss89m/PjxFenffPMNkydP5qWXXmLkyJEADB8+\nnD333JPbbruNt956qyLvli1b+Ne//kXbtm1jlrOu2GXqJIoeXxzO2oyNMdUJ79gV8vbbbzN48GBa\ntmxJRkYGvXv3BmDevHlx97XHHntUBGKo7Ci2fPnyuNsdfPDBFYE4tF1ZWVnFpeovvviC7t27VwRi\ngB49erD33nvH3e/333/P6tWrOf300yktLa14DB06lK1btzJ37tyKvJmZmRx11FG++4l+jT7//HPS\n09M55ZRTKtLS09M57bTT+OSTTyLyDh48uF4CMSQhGItIFxF5TUQ2ichmEZksIl13Yj9jRURF5JOo\n9OYi8oqIzBeRrSJSICKfi8hv6+5Z1I0ZC9bHXGfB2BhTnQ4dOkQsf/rpp4wYMYJevXrxwgsvMGPG\nDD7++GPA1XTjadWqVcRyVlZWnWy3evVq2rdvX2U7v7Rw69e778dhw4aRmZlZ8ejXrx8Ay5Yti9hX\nrI5V0a/RqlWraN26NZmZmVXy5efnV0mrL/V6mVpEmgBTgR3AKECBW4FpIrKPqm6Nt33YfnoC4wC/\neSOzgFLgDmAxkA2MBJ4Xkfaqel9tn0dd2Ly9hO9WbIq53oKxMbWXqLbahiK6Dfj111+na9euvPji\nixVp4Z2wkqFjx4589NFHVdLXrVtHx44dY27Xpk0bAJ599lnf4VzhQ6zitYVHr+vUqRP5+fmUlJRE\nBOQ1a9bQunXruNsmUn23GV8E9AT6qup8ABH5FvgZuBi4N+B+HgFeBPoS9RxUdQNwVlT+90RkD+AC\noEEE488XbiS8ubhrmyYs3bitYnlDoZsSMy3N5tk1xgRTVFRUUTMNCQ/MyXDAAQdw11138e2331Zc\nql60aBHfffdd3GC899570759e5YsWcK5555bZ+U58MADKSsr44033uCMM84AXPv066+/zpAhQ+rs\nODVV38H4JGBmKBADqOoiEfkUOJkAwVhEzgIGAr8BJtfg2BuAYIPw6sGMhZHtxUf0bc/rX62gcEcp\nAKXlyqaiElo3zfLb3BhjqjjqqKN49NFH+dOf/sQxxxzDxx9/zEsvvZTUMo0YMYI999yTU045hdtv\nv52MjAwmTJhAx44d447ZzcjIYNKkSVx00UVs3LiRo48+moyMDBYsWMAbb7zBe++9R3p6eo3Ls99+\n+3HKKadw8cUXs3Hjxore1IsXL07qD5f6bjPuD3zvkz4HqHZaGRFpjavZjlHVjdXkFRHJEJG2IjIa\nGE4DqRVD1fHFB/dqS7tmkYHXLlUbY2rilFNO4ZZbbuHFF1/kpJNO4rPPPuPNN99MapnS0tJ49913\n6d69O+eeey5XXXUVf/zjH+nVq1dFb+hYRo0axeuvv85nn33Gqaeeyqmnnsrjjz/O4MGDazX5xrPP\nPsuZZ57JjTfeyIgRI1izZg1TpkzhgAMO2Ol91pao1t/dgUSkGLhXVcdGpd8KjFXVuDV1EXkSd2n6\nUFVVEZkOZKhqlWsLInIF8IC3WAJcqaoPx9n3aGA0QNeuXX+xZMmS4E+shvK3FrP/Lf8JOzZ8Ne4o\nRj8/iy8WV3Yg+PvvDuKXvdslrBzGpJq5c+dWdPAxDdeGDRvo2bMnY8eO5brrrkt2cWot3nknIl+q\n6qDq9tFoxhmLyCHAucBADfYL4mVgJtAOd3n8AREpU9XH/DKr6uPA4wCDBg1K6C+UzxZF1or7dWxB\n66ZZtGsWOX2bTfxhjEkFDz74IDk5OfTu3btiIhJwNV/j1Hcwzgda+6S38dbF8xjwN2C5iIT60mcA\n6d5ykapWRC9VXQeE5oCb4vXkvltEnlLVkto8idryu0QNVAnGducmY0wqyMrKYtKkSSxdupT09HQO\nOuggPvzwQ3bbbbdkF63BqO9gPAfXbhxtL+CHarbt5z0u8VmXD/wRuD/O9rNww6k6APFHsSdY9GQf\nv4wZjK1mbIxp/EaPHs3o0aOTXYwGrb6D8du42mlPVV0IICLdgV8BY+NsB3CET9r9QDrwf8B8n/Xh\nDgMK8R+bXG/WbtnOz2sLK5bTBA7o0QaWfcEZc8fRK1O5veRsVtKO9XazCGOM2SXUdzB+ArgCeEtE\nxuEm/bgFWIa7DA2AiHQDFgATVXUigKpOj96ZiBTgOnBND0u7GBgMfICrAbcFzgBOw3USS+q135kL\nIzuB7717K1rsWAvPnUynkq2ckA5pKJeVXGk1Y2OM2UXUazBW1a0iMhQ3xOh5QIAPcT2dC8OyCq7G\nuzN917/DjVm+G9cWvR6YC5ygqkm/hUv0FJgH92wL/7kJSionHxuc5q7YW5uxMcbsGuq9N7WqLgVO\nrSbPYlxArm5fh/uk/Q84bieLl3DR7cXHNF8In70WkdZGCsmixGrGxhizi7C7NtWjlQVFLN5QOeVl\ndrqyz3e3++ZtTwHrC3dQn+PAjTHGJIcF43oUXSu+qu1M0tZ855s3TwooKXNTYhpjjEltFozrUfj4\n4hYUcs6252PmzZMCwIY3GbMrEZFqH9OnT6+TY/3www9MmDCBwsLC6jObhGs0M3A1dqrKzLCbQ/wx\n43WalBbEzJ8nbg6UdVuK6Z2X8OIZYxqAGTNmVPxfVFTE0KFDGTduHMcfX3krSL/bCe6MH374gZtv\nvplLLrmEZs2a1ck+zc6zYFxPlm7cxoqCIgD6yHLOSf9PZIam7WHruopFqxkbs+sZPHhwxf+hGmuv\nXr0i0huT7du3k5NT9WZ5RUVF5Obm7tQ+y8rKKC8vj7gXcSqwy9T1pLK9WBmf8SwZUl65slVXGHJV\nRP4O3uygFoyNMX4WLVrE6aefTqtWrWjatCnHH388CxYsqFivqkycOJGePXuSk5NDx44dOe6449iw\nYQNTpkzh9NNPB6BTp06ICHvuuWfc402bNo0hQ4aQm5tLu3btuPTSS9m2rbJD6qOPPoqI8NVXX3HI\nIYeQm5vLAw88wI8//oiI8Morr3DWWWfRsmXLimOXlpZyww030KVLF7Kzs9l777159dVXI4575pln\nMmTIEF555RX69etHdnY233zzTV29jA2G1YzrSej+xcPTZjEkfU7kyqNvg7TI+3JazdiYOjChZbJL\n4EzYVKe7W7t2Lb/61a/o3LkzTz75JFlZWdx2220cffTRzJ07l6ysLJ544gnuuece/vznP9OvXz/W\nrVvHBx98QFFREQcffDC33347119/Pe+++y5t2rSJW1OdOnUqw4cPZ+TIkdxwww2sWbOGsWPHsmXL\nFl544YWIvCNHjuTyyy9n4sSJtGnTpiL9yiuv5IwzzuD1118nI8OFnmuvvZYHH3yQm2++mf3335+X\nXnqJM844g8mTJzNixIiKbefNm8dNN93ETTfdRLt27ejSpUudvp4NgQXjeqCq/G/BBrIp5oaMyBOX\nHodCvxNh5VcRyR28NuP1W2ziD2NMpEmTJlFeXs6HH35YcU/ggw8+mB49evD8889z4YUX8vnnn3PC\nCSdw8cUXV2x36qmVUzz06dMHgIEDB9KxY8e4x7v22ms58sgjIwJvXl4eJ554IuPHj6/YF8A111wT\nccwff/wRgMMOO4z776+8fcCaNWt46KGHmDhxItdeey0Aw4cPZ8mSJUyYMCEiGK9fv56PPvoopW+P\naZep68GCdVtZt2UHv0t/j65ple3CSDocc5e7oXGzyA9De6sZG2Ni+OCDDzjmmGNo0qQJpaWllJaW\n0rp1a/bdd19mzZoFwH777cebb77JxIkTmTVrFuXl5dXs1V9BQQFffvklZ5xxRsWxSktLOeywwwD4\n6qvIikR4Z7N46bNnz2bHjh0Vl6xDRo4cybfffsvmzZsr0nr27JnSgRgsGNeLGQvW05ENXJ7xVuSK\nA34HHbyAxt4OAAAgAElEQVSekc3yCJ90rJ1sJoNSC8bGmCrWr1/Ps88+S2ZmZsTjf//7H8uWLQPg\n0ksvZfz48bz44osccMABdOzYkZtvvrnGQXnDhg2oKhdccEHEsZo1a0Z5eXnF8UI6dOjgu5/o9FWr\nVvmmh5bz8/OrpKUyu0xdD2Ys3MB1mf+giYQF1tw2cMR1lcvpmdC0XUSP6vZsYt0WG3JgzE6r47ba\nhqJNmzYMHjy44vJuuJYtXTt5eno6Y8aMYcyYMSxZsoTnnnuO8ePH061bN84777zAx2rd2t2C/o47\n7uDII4+ssn733XePWBbxn8k4Or1Tp06Aa//u0aNHRfqaNWsijhtvn6nEgnGClZcrRfM/4eT0/0Wu\nGHYj5LaOTGveMWp4Uz5zC9ujqrvEyWiMCWbYsGFMmTKFffbZh6ysrGrzd+vWjRtvvJEnn3ySH35w\nN6IJbbd9+/a427Zp04b999+fn3/+mbFjq7vTbXD77rsv2dnZvPrqq4wZM6Yi/ZVXXmGfffapaAvf\nVVgwTrCfVhVwddlTEQ0C2mFvZOCoqpmbdcTddMrpIPnMLitn8/ZSWuam1pg6Y8zOGzNmDC+99BLD\nhg3j8ssvp1OnTqxevZrp06dz5JFHcuqpp3L++efTuXNnDjzwQFq0aMH777/PsmXLGDp0KEDFUKaH\nH36YU089lWbNmtG/f3/f402aNIljjz2W8vJyTjnlFJo2bcrixYt55513uO++++jWrVuNn0OHDh24\n/PLLuemmmwAXnF9++WWmTp3K5MmTd/KVabwsGCfYhk/+xpC0xRFpctyfqwxlAqB5ZLtI+PAmC8bG\nmJCOHTvy2WefccMNN/D73/+ezZs306lTJw499FAGDBgAwC9/+UueeuopHnroIYqLi+nTpw/PPPMM\nxxxzDAB77LEHt99+O4888gj33HMPffr0qej5HG3YsGFMmzaNCRMmcPbZZ1NeXk63bt049thjadu2\n7U4/j7vuuoucnBz++te/snbtWvr27cvLL78c0ZN6VyF2V6CqBg0apKEeibVSlM+WSfvSvLyy3WpB\nh2PodenL/vmn3gofT6pY/EvpCO4rPZ2XRw/moJ47f8Ibs6uYO3duyve6NQ1PvPNORL5U1UHV7cN6\nUydQ+fQ7IwJxkWZRNuzm2Bs0i+pVWDELl401NsaYVGbBOFHW/oh8/kRE0tNpI+jdu2/sbZp3ili0\nWbiMMWbXYME4EVRhyrWIllUkLStvz489ziMtLU6v6OaRE39UzMJlwdgYY1KaBeNE+PFdWDg9IunW\n0rM5oM9u8bdr5t+Ba90WC8bGGJPKLBjXtZLt8O/rI5I+LevPv8sP4OBe1XTCigrGbdlMOmVWMzam\nBqxTqqlPdXW+WTCuawunQ8GSisVSTePm0nNp3zyHXu2rmU0rIwuaVAbsNFHasYl11oHLmEAyMzMp\nKipKdjHMLqSoqKhO7q1swbiu9T0GLprK6uZ7A/B82VHM0y4c3LNtsFm0fDpxrbfL1MYEkpeXx4oV\nK9i2bZvVkE1CqSrbtm1jxYoV5OXl1Xp/NulHInT+BVc3/zN5G97mw/L9Aaq/RB3SrAOs+b5isYPk\nM69wh02JaUwAoSkUV65cSUlJSZJLY1JdZmYmHTp0qJOpO6sNxiKSBawGzlPVt2t9xF3A9pIyZi3d\nxI7yQyrSfhk0GPvUjHeUllO4o5TmOTYLlzHVadGixS43r7Fp/Kq9TK2qxUApEH82cVPh66UugIbs\n1jKHrm2aBNs4akrMyuFN1m5sjDGpKmib8ZvAaYksSCqZsXBDxPLBvdoFv8TcLHKscXtsrLExxqS6\noG3G/wL+KiKv4QLzKiCid4SqTq3jsjVaMxasj1gO3F4MVSb+qJiFyzpxGWNMygoajF/3/p7iPUIU\nEO+vz22Idj3bikv5ZllBRFptgnHoMvU6qxkbY0zKChqMj0hoKVLIjpJyLhjSg5kLNvDdik10adOE\nzq1yg+/AasbGGLPLCRSMVfWjujqgiHQB7gOOwtWqPwCuVNWlNdzPWOAO4FNVHRKWvgdwOe4HRE9g\nC/AFcKOqzq6TJxFH66ZZXHesu5XWpqISVhbUcAKCqFm42rGJNMpt4g9jjElhNRpnLCJtgIOBNsBG\nYIaqbqzB9k2AqcAOYBTu8vatwDQR2UdVtwbcT09gHLDWZ/XRuED8LPAV0AoYA8wUkSGq+mXQ8tZW\ny9xMWubWcDhSRjbktoYid3k6XZS2bLIOXMYYk8ICB2MRuRW4GsjC1WgBdojI3ap6Y8DdXISrrfZV\n1fnefr8FfgYuBu4NuJ9HgBeBvlR9Di8BD2nY9DsiMhVYDPwBODfgMZKneaeKYAzeLFwWjI0xJmUF\nGtokIlcC1wMvAEOBfrja5wvA9SLy+4DHOwmYGQrEAKq6CPgUODlgWc4CBgLX+a1X1fUaNQ+eqm4C\n5gGdA5YzuZpVHWtswdgYY1JX0HHGlwB/UdWLVPUjVf3J+3sR8FfgsoD76Q9875M+B9iruo1FpDWu\nvXlMDS+PtwEGAHODbpNUvvNTW5uxMcakqqDBuDvwbox173rrg2gD5PukbwRaB9h+Eq6G+0zA44U8\ngLu0fn+sDCIyWkRmicisdevW1XD3dSx6Fi7yKSopY+uO0iQVyBhjTCIFDcYbcDVLP/299QklIofg\n2nsvjb4MXc121wFnAVeEXx6PpqqPq+ogVR3Uvn372he4NnxqxmCzcBljTKoKGozfAG4RkXNEJANA\nRDJE5DfARConBalOPv414Fg15nCPAX8DlotIKxFpheu8le4tZ0dvICKXALcD41T1qYBlTL6oNuO8\n0MQfNtbYGGNSUtBgfB3wDW64UJGIrAGKcD2aZ+M6dwUxB1eTjrYX8EM12/bDtV3nhz1+BQz2/r80\nPLOInAM8DNyjqrcFLF/DEGviD6sZG2NMSgo66ccWETkUOB44hMpxxh8B/6rBZeO3gbtFpKeqLgQQ\nke64oDq2mm39ZgG7HzcN5/8BFZegRWQE8DTwpKpeE7BsDUeMYGwTfxhjTGoKej/jS4EPVfUd4J1a\nHO8J4ArgLREZh5v04xZgGe4ydOiY3YAFwERVnQigqtN9ylYAZISv8340/ANXY39GRAaHbbJDVb+u\nRfnrR5U7NxWQRrlNiWmMMSmq2mCsqsUicicwvLYHU9WtIjIUNzzpeVwP5w9x02EWhmUVXI036GX0\ncEOBbNxY5E+j1i0heM/v5MnMgZxWsN3ViDOknDZsscvUxhiTooLOwDUXN3PWx7U9oDcH9anV5FlM\n5Sxf8fId7pM2AZiwU4VrSJp3rAjGYBN/GGNMKgta87wJuFFE9k5kYUyYqB7V7SWf9dZmbIwxKSlo\nzfhaoBnwtYgsBlbh2ntDVFUPq+Oy7dp8xhovspqxMcakpKDBuIzqhx6ZuuQzC5d14DLGmNQUdGjT\n4Qkuh4nmUzPeWlzGtuJSmmTV6M6XxhhjGrhq24xFJEtE3vCGDJn64nPnJsBuGGGMMSmo2mCsqsXA\nkUHymjoUY37qddZubIwxKSdogP0UN+2kqS/N/eentuFNxhiTeoI2Pl4NvCkihcCbVO1NjaqW13HZ\ndm1VZuHahFBuwdgYY1JQ0Jrxd0Av4C+4WayKgZKwhzVk1rWsJpDdsmIxU8poTaG1GRtjTAoKWjOe\nSFRN2NSD5h1gx6aKRZuFyxhjUlPQoU0TElwO46d5R1g/r2IxTwosGBtjTAqqcQ9pEWkmIt1EJDMR\nBTJhmkXfStFqxsYYk4oCB2MROUFEvgI2AQuBvb30J0XkrASVb9cW3aOaAtbZLFzGGJNyAgVjEfk1\n8BawHjdPdfgdlRYBo+q+aKbqWGO7WYQxxqSioDXj8cDTqno0cH/Uuu+BAXVaKuNUmYWrgMIdpWwv\nKUtSgYwxxiRC0GDcD3jZ+z+6V3U+0LbOSmQq+dSMAbtUbYwxKSZoMN4MtIuxrjuwrk5KYyI1j+7A\n5abEtE5cxhiTWoIG4/8A14lIq7A0FZFs4ArgX3VeMlPlMnUe+YBau7ExxqSYoJN+3AB8DvwEvIe7\nVD0W2AdoCfw6IaXb1WU3g6zmULwFgCwpozVbrGZsjDEpJlDNWFUXAwOBd4CjgDLgUGAmcJCqrkxU\nAXd5VW4YUcB6azM2xpiUEvgu9aq6HLgwgWUxfpp3gg3zKxZtFi5jjEk9do/ihq7K8KZ8u6exMcak\nGAvGDV10j2oK7M5NxhiTYiwYN3RVhjfZ/NTGGJNqLBg3dFVuFlFgl6mNMSbFWDBu6Hwm/tiy3abE\nNMaYVGLBuKGLCsYdcFNibthq7cbGGJMqAg9tEpGewBlAVyAnarWqqg17SgTfKTGV9Vt20LlVbnLK\nZIwxpk4FCsbeLRRfwdWk1wLRjZbRN48wdSW7OWQ2hZKtblFKaMlW68RljDEpJOhl6luA6UAnVd1N\nVXtEPXoGPaCIdBGR10Rkk4hsFpHJItK1pgUXkbEioiLyic+6q0TknyKyysszoab7b1B8ascWjI0x\nJnUEDcY9gbtVtVZ3ZxKRJsBUYE9gFHAO0AeYJiJNa7CfnsA4XC3dz0VAHvBmbcrbYPgMb7LbKBpj\nTOoI2mb8I3Vzz+KLcIG9r6rOBxCRb4GfgYuBewPu5xHgRaAv/s+hv6qWi0gGcEmtS51s0bNwkW93\nbjLGmBQStGY8Brjeq5HWxknAzFAgBlDVRcCnwMlBdiAiZ+FuWnFdrDyqWl7LcjYszTtFLNpYY2OM\nSS1Ba8YTcDXjuSLyM7Axar2q6mEB9tMfeMsnfQ5wenUbi0hr4D5gjKpuFJEAh0wBzavOT/21XaY2\nxpiUETQYl+HuZVxbbcAbKBtpI9A6wPaTgHnAM3VQlggiMhoYDdC1a437kyVWVM24vXXgMsaYlBIo\nGKvq4QkuR7VE5BDgXGCgqtb5UCpVfRx4HGDQoEENa6iWz52brM3YGGNSR33PwJWPfw04Vo053GPA\n34DlItJKRFrhfkyke8vZdVvUBsTnzk2bikooLk2tpnFjjNlVBQ7GItJJRO4WkS9EZIH3988i0rH6\nrSvMwbUbR9sL+KGabfvhekbnhz1+BQz2/r+0BuVoXGLMwrVhq12qNsaYVBAoGIvIHsA3wO+BQuBz\n7+8fgG9EpE/A470NDA7vlS0i3XFB9e1qtj3C5zEb+N77/7WAZWh8sltARuXUl7lSTAu22X2NjTEm\nRQTtwHUXsBk4SFUXhxJFpBvwvrf+lAD7eQK4AnhLRMbhptG8BViGuwwdvt8FwERVnQigqtOjdyYi\nBUBG9DoRGQR0p/LHxl4icpr3/3uqui1AWRsOEVc7zl9UkdReClhXuB1ombxyGWOMqRNBL1MfAdwY\nHogBVHUJbtjTEUF2oqpbgaG4HtHP4ybuWAQMVdXCsKwCpNegfNGuAF4FXvaWT/eWX8XNzNX4RN+9\nSfKtZmyMMSkiaM04C9gSY90Wb30gqroUOLWaPItxAbm6fR0eI/084LygZWoUfDpx2cQfxhiTGoLW\nPL8B/k9EIvKLm3XjMm+9SaRmVeentrHGxhiTGoLWjCcC7+Bm4HoZWAV0xF3+7QMcn5jimQpVZuEq\n4Bsba2yMMSkh6KQfU0TkBOBW4AbcJWQFvgROUNX3E1dEA/jMT53PepsS0xhjUkLQmjGqOgWY4t0G\nsTWQ3+h6JTdmUbNw2T2NjTEmdQQOxiFeALYgXN+ia8ZYm7ExxqSKmMFYRG4CnlTVld7/8aiq3lK3\nRTMRfNqM87cVU1JWTmZ6fc9qaowxpi7FqxlPAKYAK73/4wlN3mESJacVpGdDmasNN5EdNKOIDYXF\ndGyZk+TCGWOMqY2YwVhV0/z+N0kSmoWrYElFUqjd2IKxMcY0bkHnpu4qIpkx1mWISAO7AXCK8pmF\nyyb+MMaYxi9ojXcRsH+Mdft6602iRQXj9hTY8CZjjEkBQYNxvKkpMwG7sW59aOYzP7VN/GGMMY1e\nvN7UrYA2YUmdw2996MkFRgGrE1A2E83nvsar7TK1McY0evF6U/8BGI/rKa3Evl+wePlMovkE4+8t\nGBtjTKMXLxi/CSzGBduncFNhLojKswP4QVW/TUjpTKRm0WONbeIPY4xJBfGGNs0GZgOIiALvqOqG\n+iqY8RE1C1d7ClhnHbiMMabRC3qjiGcTXRATgM/QJuvAZYwxjV/gualFpD/wO6AvED3LhKrqsLos\nmPGR2xpNz0LKXABuJtsp3raJ0rJyMmxKTGOMabSCTvpxEO52iccCw3F3beoJHA70Jv7QJ1NXRJBm\nVccab9xqtWNjjGnMglanbgcmA/1xgfdCVe0OHAmk4zp3mfoQdcOIPApsFi5jjGnkggbjfYAXcEOc\nwAVgVHUqLhDfUfdFM76qDG+ydmNjjGnsggbjLGCrqpYDG4Hwbr0/AQPqumAmhmY+wdh6VBtjTKMW\nNBjPBzp7/38LXCAiaSKSBpyPzcBVf3wm/rCxxsYY07gFDcb/xHXWAtd+fCywGcgHzgLurfOSGX++\nw5ssGBtjTGMWdJzxhLD/PxCRwcCpQBNgiqq+n5jimSqia8YUMN0uUxtjTKMWeJxxOFX9Gvi6jsti\ngqjSZlxgHbiMMaaRCzrOeLCInBFj3eneOGRTH3x7U1vN2BhjGrOgbcZ34MYY++mHDW2qP7lt0LTM\nisUWUkThlk1JLJAxxpjaChqM9wVmxlj3OW4csqkPaWnQLC8iKXPbWsrKNcYGxhhjGrqgwTgnTt50\noGndFMcEIVF3b2pnU2IaY0yjFjQYzwVOirHuJNzEH4GISBcReU1ENonIZhGZLCJdg24ftp+xIqIi\n8onPujQRuU5EFovIdhGZLSKn1vQYDZYNbzLGmJQSNBg/ClwkIpNEZA8RaSIifURkEnAh8HCQnYhI\nE2AqsCcwCjgH6ANME5HAtWsR6QmMA9bGyHILMAF4EDcmeibwqogcF/QYDVqzqPmpbeIPY4xp1IKO\nM35CRPoCfwSuCl8F3Keqjwc83kW4uz31VdX5ACLyLfAzcDHBJw95BHgRdzvHiOcgInnANcCdqnq3\nlzxNRHoDdwLvBTxGwxV1mTpPCvh43jq2l5QH2rxjixwGdG6BiN1syxhjGoLA44xV9RoReQR3p6a2\nwHrgA1VdWIPjnQTMDAVib7+LRORT4GQCBGMROQsYCPwGdyepaMNxc2m/EJX+AvCUiPRQ1UU1KHPD\nE33nJsnnzv8u4on/Bn9aw/t34LFzBtV1yYwxxuyEGk36oaoLgAW1OF5/4C2f9DnA6dVtLCKtgfuA\nMaq6MUbNrj+wAzefdvQxAPYCGnkwjqoZk1/jXfx7zhpmLytg3y6t6qpUxhhjdlLMYOx1qlqlqiVB\nOlip6tIAx2sDvpFjI9A6wPaTgHnAM9Uco0BVo8f6bAxbX4WIjAZGA3TtWuP+ZPUrqs24gxTs1G6+\nsWBsjDENQrya8WJgMG4c8WIq72UcS3rdFMmfiBwCnAsM9Am0tea1ez8OMGjQoIY9aDeqZrxbxiaO\n7JUXI3Ol5flF/Lh6S8Xy9ytsshBjjGkI4gXj86m8JH0B1QfjIPLxrwHHqjGHewz4G7BcRELVuQwg\n3VsuUtUd3n5aiYhEBe1QjXgjjV2TtpCWAeWlADQtL+TJswZAZm7czT6dv56zn/ysYvk7C8bGGNMg\nxAvGLams7U7Fu2Rdy+PNwX9azb2AH6rZtp/3uMRnXT6up/f93jGygV5Ethvv5f2t7jgNX1oaNM2D\nLSsr07ashjY94m7Wf7cWEcs/ry1ke0kZOZkJvahhjDGmGvHGGd8HdPf+XwTsXwfHexsY7I0TBkBE\nugO/8tbFc4TPYzbwvff/a16+KUAJcHbU9r8Fvm/0PalDoib+oHBNtZu0apJFlzaVteeycuWnsMvW\nxhhjkiNezbgACH3jC3VzmfoJ4ArgLREZ5+3zFmAZ7jK0O5hIN9wl8omqOhFAVadH70xECoCM8HWq\nulZE7gWuE5EtwFfASGAosWcRa3yig/HyWZDVrNrNhrddzzsbt7OatoC7VG2duIwxJrniBeNPgWdF\nZLa3/IiIbI6RV1V1WHUHU9WtIjIUV+t+HhfkPwSuVNXCsKyCu0QedIawaDcAhcAfcD8ofgLOUNV3\ndnJ/DU90MH7/hkCbjQPG5cBrZYdyTcklzFlp7cbGGJNs8YLxRcB43NSV6uXNjJM/EG8IVNx5olV1\nMS4gV7evw2OklwG3eo/U1Kxj9XniOC39Yx4rPYHvVrSoPrMxxpiEihmMVXUNcBmAiJQDo1X18/oq\nmKlGlwNrvYsBsoh3VnehuLScrIydvQhhjDGmtoLOwNUDWJXIgpga6nk4DL8Dvn8NSrYH22bb+oiO\nXnukLaekVJm3ZgsDOrdMSDGNMcZUL+iNIpYkuiCmhkTg4MvcI6jZL8MboysWe8sKwE3+YcHYGGOS\nJ+a1SREpE5EDvf/LveVYj9L6K7LZaXl7Riz2CQVj68RljDFJFa9mPBFYHvZ/w54i0lSvbR/CR6l1\nlbXksIPvVsTqJG+MMaY+xOvAdXPY/xPqpTQmsbKaQOtukL8YgDRReskq5q7KpaSsnMx068RljDHJ\nsNPfviLSRkR+ISLZdVkgk2Dt+0Us9pHlFJeWM39tYYwNjDHGJFqgYCwi40TkjrDlQ3F3cvoc+FlE\n+iSmeKbOte8bsdgnzbVE2B2cjDEmeYLWjH8LLAxbvgs3L/SvgTW4KS1NY9A+shPXHmE9qo0xxiRH\n0HHGnYGfAUSkPXAgMExVp4tIFvDXBJXP1LWoHtW9xasZr7ROXMYYkyxBa8ZlQJb3/6HAdtzc1QDr\nqLxXsGno2u0RsdhV1pJNMT+s3ExZuXWYN8aYZAgajOcAvxWRZsAFwEdh9zbuAqxNROFMAmQ1hVbd\nKhbTReklKykqKWPhOuvEZYwxyRA0GE8EzgA2AcNwbcYhx+FuU2gai/bRl6pt8g9jjEmmQMFYVf8N\n9MMF5P6q+lHY6o+JDM6moYvqUb2H16P6u+XWbmyMMckQtAMXqroIWOST/lidlsgkXl70WGOrGRtj\nTDIFHWd8soicH7bcTURmiMgWEXnNa0s2jUVUzTh0mfqHlZspt05cxhhT74K2GY8D2oct3wvsDjyO\n6109oW6LZRKqXWQw7i6ryaaYwh2lLN6wNUmFMsaYXVfQYNwL+BZARHJxnbauUtWrgeuBEYkpnkmI\n7GbQsmvFYrooPWQ1YOONjTEmGYIG4xygyPv/l7i25ve95Z+A3eq4XCbRojtxiU2LaYwxyRI0GC8G\nhnj/nwx8qaqhb+083JAn05hEz8Rlc1QbY0zSBO1N/Rhwt4iMAPYDLg1bdzDwQ10XzCRY1FjjPmFz\nVKsqIpKMUhljzC4pUDBW1b+IyHpgMPBXVX0ubHVz4OlEFM4kUNStFEOXqTdvL2XZxiK6tm2SjFIZ\nY8wuqSbjjF8EXvRJv7hOS2TqR/vIOaq7yRqyKKGYTL5fucmCsTHG1KOgbcYm1WQ3hxa7VyxmSDk9\nZBUA31m7sTHG1KvAwVhERovI1yKyTUTKoh+JLKRJkLzY7cbGGGPqT9AZuM4FHgC+wA1zehp4AdgM\nLMDdSMI0NtGduNIiO3EZY4ypH0FrxlcCd1DZi/phVR0F9MSNP96QgLKZRKvSo9p14srfVsLKTduT\nUSJjjNklBQ3GfXB3Zyr3HlkAqpoP3Ab8ISGlM4kVY3gTwHfL7VK1McbUl6DBuAhIU3ftcjWuRhxS\niM3A1ThF9ajuLqvJpBSAOXYHJ2OMqTdBg/F3QG/v//8C14vIwSJyAO4mET8GPaCIdPHu9LRJRDaL\nyGQR6Rpgu24i8paILBGRIhFZLyIfichxPnl7eMcoEJGtIjJNRAYFLeMuI6cltOhcsZgpZXQPzVFt\nnbiMMabeBA3GjwOtvf9vBJoBnwAzgT2Aq4PsRESaAFOBPYFRwDm4S+DTRKRpNZs3A9bj7iB1HHAh\nsAV4V0ROCTtGW69sA4CLgTO9VdNEJHKmC1NljupQu/F3KzZbJy5jjKknQWfgejns//ki0h83DWYT\n4H+quj7g8S7CXeLuq6rzAUTkW+BnXOC8N04Z5uACcAUReRdYBJwPTPaSLwU6AIeq6gIv31RgIXAz\ncEbAsu4a2veDBVMrFvdIW8575bC+cAdrt+ygQ4ucJBbOGGN2DTs16YeqblXVD1T17RoEYoCTgJmh\nQOztaxHwKe4GFDUtRynuJhWlYcmDgZ9DgThUXtzl9RNEJPCsY7uEqJpxb+vEZYwx9S5mYArSjhtO\nVZcGyNYfeMsnfQ5wepDjiEga7kdEO2A07jJ5eG/uMqDYZ9MdQC7u3sw/BTnWLiGqR3VojmqA71du\n4si9OtR3iYwxZpcTr5a4GKhJo2F6gDxtgHyf9I1UtklX589UtlEXAmeq6odh638CjhKRtqq6ASoC\n+IFhZahCREbjgjtdu9bod0jjFlUz7iGryaCUUjKsE5cxxtSTeMH4AmoWjOvL/cBLQEfgXODvInKa\nqr7jrX8U+D3wnIj8HtgG3AD08NaX++1UVR/HdVRj0KBBDfF5J0ZuK2jeCba4eakzpYxusoYF2pnv\nV2xOcuGMMWbXEDMYq+ozCThePv414Fg15ipUdTkQupb6johMB+4G3vHWLxSRs4GHgFDb9FfAfcA1\nwKqdLXzKar9nRTAGd6l6gXZm9ebtrNuyg/bNs5NYOGOMSX0xO3CJc6KIDIiTZ28RObEGx5uDazeO\nthfwQw32E24WlWOgAVDV14HO3n57q+ovcEOjlgVs2961xJmJ63ub/MMYYxIuXm/qc4B/AFvj5NkC\n/ENEfhPweG8Dg0WkYgYvEekO/MpbVyNeW/AQ3M0qIqhqmarOVdUFIrIbMBJ4pKbH2CVEtRvvkVbZ\niWuOtRsbY0zCxQvGvwWe9oYe+VLVxcDfcBN4BPEErmPYWyJysoichOtdvQx4LJTJm22rVERuCkub\nICJ/FZGRInKYiIwEpuA6Zo0Py5cpIveJyK9FZKiI/B+u9jwHuCdgOXcteZFzoUQMb7JgbIwxCRev\nA3c6cnMAACAASURBVNdA3G0Tq/MBcHaQg6nqVhEZimu/fR4Q4EPgSlUtDMsquN7Z4T8WvsLdPepM\noCVujuzZwCGq+mn4YXCzep0FtMK1Lz8F3K6qfkOeTFTNuKesJJ0yyki3TlzGGFMP4gXj5gTrVJXv\n5Q3Ea7M9tZo8i3EBOTztbQJcyvYmAjkhaHkMkNsamnWEQjcvdZY3R/UC7cyKgiLytxbTumlWkgtp\njDGpK95l6vVAtwD76OrlNY1ZnJm4rBOXMcYkVrxg/AnB2oLP8/KaxizeTFx2qdoYYxIqXjC+Hxjm\ndYaqco3S6yh1PxBqAzaNWV7U8Ka0sJqxdeIyxpiEijfpxwwRuRrXA/lsEXkfWOKt7gYcBbQFrlbV\nmQkvqUksG2tsjDFJE/cORqp6v4h8BVwLjMDdaAGgCJgO3Kmq/01oCU39iArG4T2ql2zYxqaiElrm\nZiapcMYYk9qqvYWiqn6sqsfjekx39B4tVPV4C8QppEkbaJpXsZgtpXSVtRXLc6x2bIwxCRP4fsaq\nWq6qa71HWSILZZIkeiYuCZ+JyzpxGWNMogQOxmYXYDNxGWNMUlgwNpWiasZ9wuaotk5cxhiTOBaM\nTaUqY40ra8aL1m+lcEdpfZfIGGN2CRaMTaX2UZep01aSRjkAqvDDSms3NsaYRLBgbCo1bQtN2lUs\nZlFCV1lTsWztxsYYkxhxxxmbXVBeP1hcOWKtj6xgsXYC4L7/zOOFmUvIzkgjJzOdnEzvb0bY/96j\nWXY6Q/q0Z78urRJSzM3bS3h/zhqaZKUzrF8e2RnpCTmOMcbUBwvGJlL7vlWC8X8YBEDhjtIatRvf\n/f48Ttx3N248vh95LXLqpHiqytuzV3LLO3NZX7gDgJ7tm3LryQP4Ze921WxdM18uyefBqT+zZOM2\njt+7E5cc1oum2faRMcbUPftmMZGiOnHtlbECajGq/J+zVzL9x7VcM7wvvx3cjfQ0qX6jGBauK+TG\nt77n0/kbotK3ctaTn/Hr/XbjhuP3on3z7J0vMLCioIi7/vUjb89eWZH2wNT5vPzFMsYeuye/3q8z\nabV4HsYYE01UNdllaHAGDRqks2bNSnYxkmPRf+HZyttBF7Tsx7DCW9iwtbjWu967c0tuGzGAfXav\n2aXr7SVlPDx9AY9OX0BxWXncvM1zMhhzzJ6cdWDXGgf+rTtKeeyjBTz28UJ2lMY+zn5dWjH+xL3Y\nv2vrGu3fGLPrEZEvVXVQtfksGFe1SwfjwnVwd+/K5YwcSq9dzqYd5WwvLWd7SVnYozzyb2nl/+99\nt4o5Pr2vReCcwd24+ui+gea6/mjeOm5663uWbNhWZV1mulBS5n/+7tulFbf9egADOres9hjl/9/e\nmYfHVVx5+z3drdZiSZZkyZYtI8u2vGC8YYzBG2vAEMAkBJKQDEsCYUkyDAnJhCwQIPCRLwNJJpPJ\nlzAESAIJSSbsBrOb2AYbjDE23vAmywuSrc2WrbW76/ujbkutVmvvRct5n+c+9966deuearX6d+vU\nqaqA4ekPDvCzl7dRfrSxy/xBLptTwPcumMqoKLngFUUZfKgY94EhLcYAP5sAdSGu4Fs+gJwJPSrC\n5w/wpzV7efCVjyP2M+emJ3PHxSeydNYYRNq3YMuPNnDPC1tYtvGTiOUvLB7BTy6dTnVdMz98ehPb\nymrb5XEJXD2/iNvOn0xGSmThX1dSxT0vbGHj/siR4tNGZ3LahByeWFtKU4TWcprXzTfOLua6ReNJ\nSdIgMkVR2qJi3AeGvBg/+mnYu7r1/MonYcqFvSqqu6I6IS8d6J2I+/wBHnu7hF+8+jHHm9p3cI/M\nSObOS6Zx0YzRLffsr67jpy9t44UO7MpNT+a7SyZz+Skn4HYJpZV1/J8Xt7J8c1nE/IU5afzwohM5\nf9qoiC8XiqIMTVSM+8CQF+MXvgXrHmk9P/fHsPjbfSrynx8f5o4O3M1et4ubzpzAokl53P385l67\ntz85Us89z2/hpY8iC+YZk/O4/YKpvLjpE/5nZeR+Ya/bxXWLx/P1syZGbE2v3lnBPc9vYXt5+5Y4\n2JeLOy8+iSn5GRGvK4oytFAx7gNDXozXPgQvfbf1fOYX4bLf9bnYngRihTKjYDj3fmY6s7o5ZvnN\nbYe487mP2FdV3yP7Pj0jn+9feCIn5KR1ms/nD/CXd0t58NWPqalrbnfd7RK+fFohN505kTFZqRFK\nUBRlqKBi3AeGvBjvfgv+uLT1fPQsuPGf0Sv+8DHufHYzq3ZWdJovI9nT6yFR9U1+fv3mDh765+4O\ng7yCTC/I5I6LpnHahBE9ekZNXZOdCGVtKf5A+2e4XcKSk0Zxzfwi5o3PUfe1ogxBVIz7wJAX42OH\n4IFJreeeVPjBQXBFb/bU4OQd9y7byuHa9hHM0ZosZOehWn70zEes2V3V7lpeRjL/vmQKn5sztk/j\nhreX1XLPC5vbjX8O5cTRmVy7YByXzi7QQC9FGUKoGPeBIS/GxsDPxkN9dWvaLRsgZ3zUH3W0oZkH\nX97OH9fsxRgoGpHGTz4zncWT8qL2DGPs0KX7lm2l8ngTXo+LGxZP4KazJpIepRm1jDG8sqWc+5Zt\npbSqfb94kKy0JL54aiFXzR9HgbqwFWXQo2LcB4a8GAM8cgGUvtN6fuVfYcoFMXvcoaMNHKipZ0bB\ncDzu2Kxf0tDsZ9OBI0wamU5Wmjdmz3hq/QEee3sPH5cf6zCfS+D8aflcs6CI0yeoC1tRBisqxn1A\nxRh4/lZ4/9HW80/dDYtuTZw9AwxjDO/sruSx1SW8trWcCF3KLUzNz+CaBUXMKBhOkz9Ak89uzcFj\nf4DG8DRfgCSPi3njc5g9Nkun51SUfkp3xVjnplYiEzZHNYe3JcaOAYqIsGBiLgsm5rKvqo7H1+zl\nyff2caS+ffT1trJavv/Upl4/Kz8zhSUnjWLJ9HzmFeXEzLOgKErs0JZxBLRlDOxeAX+8tPV8zMlw\nw4oEGTM4qG/y88yGA/zh7ZKIM4ZFg+y0JM6bNooLpuezsDhXl5ZUlASjbuo+oGIM1JbBg1Naz5PS\n4PsHohpRPVQxxrB2TxWPrS7hlS1lnbqw+0J6soezp47kwun5nDk5T5d/VJQE0G/d1CJyAvAL4DxA\ngNeAW40xpV3cNw74FTAbGAkcBzYD/9cY82JY3kLgJ8DZQB6wD/gbcL8x5nhUKzRYSR8FKcOhwZmz\nubkOjuyD7HGJtWsQICKcPmEEp08Ywf7qOh5fU8rbuypo9hu8Hhdetzh7l9173CS5heQ2aS72VBxn\nxfbD1EWYAhTs+tPPf3iQ5z88SLLHxRmT8zhtfA7JSW48LrGbW/C4XM6xqyXN7RKS3C5SPG5OHJ2h\nrm9FiTFxFWMRSQPeABqBawAD3Au8KSIzuxDKdKAC+BGwH8gEvgYsE5HPGWOecp4xDCvwScAdQClw\nKnA3MAn4QgyqNvgQgbwTYd+a1rTD21WMo8zY7DRuv3Bq1xk7oKHZz8odFSz/qIzXtpZH7JMGaPQF\neHVLOa9uKe/xM3LTvdz7melcMH10r+1UFKVz4t0y/howAZhijNkJICIbgR3AjcDPO7rRGLMZuC40\nTUSWAXuArwBPOckLsaK7xBjzipP2pojkAN8RkTRjTMcDQZVW8qaEifFWmHx+x/kDfmiuB18DBHww\nLA9cMeyzbKoDcUHS0F3CMCXJzXnTRnHetFE0+wOs2V3J8o/KeHlzORXHur8cZGdUHGvipsfX89WF\n47n9wql4PdpKVpRoE28xXgqsCQoxgDFmj4isBi6lEzGOhDHGJyJHgNDlfYIDSMNXG6gBXFjXuNId\nRp7Y9nztQ7B9OfjqregGhbe5wab5m9rmTx4O4xZA0SK75c/omzgfr7SrSZWsstuhzSBuG1xWtAiK\nFkPhaZA8NBdpSHK7WDwpj8WT8rjn0ul8UFrN8o/KWL65jP3VPZunOxKPrN7D+tJq/vvLc3TCEkWJ\nMnEN4BKRMuBZY8yNYem/Aa4wxnQ57ZKIuLCimgvcgHVbX2iMed25ngJsBD4Bbsa6qecBTwBPG2O+\n3tUzNIDLYdcb8KfPRq+8nopzJPHtir6IszFwrBwqd0HVLqjabY8bayHrBLumc85EGDERsseDt/MF\nJfoLxhg2HzzKm9sOUXm8iWZ/AJ/f4AsYfIGA3fsD+AOGZr+T5lzfuL+m3dzeWWlJ/Pzzszhn6qgE\n1ah/UtfkY9eh42SlJTE2O1UnclGAfhpNLSJNwM+NMbeHpd8L3G6M6bKlLiIPALc5p8eAa4L9xSF5\nRgL/ABaFJD8M3GiMibhckIjcgBV3CgsLT9m7d2/3KjWYqS2HByfHrvxwcc4ssLN+9UR8uyJcnE+Y\nZ4PRKh2xrdrlHO+x5809iO/LGGOFOWe8FemcCQNOqLvig9JqvvnnDzhQ075lffNZE7ntvMlDNrjL\nHzBsOnCEVTsOs2pnBev31rSsRpabnszJhVnMKcxmTmEWM8dmkerVYWZDkcEsxmOBfGe7Guv6vtwY\n84JzPQV4CRiDjagOtozvBJ4wxtzc1TO0ZRzCUzfCxie7n9+Tavtw/T5ois1Y2lYEGwPYDxkxCeZc\nBSdfBWk50SvX1wRbnoF1j0LNXph0Piz4V/sSECNq6pq47W8f8vq2Q+2uzSvK4b++dDKj+rigx0DA\nGENJZR2rdlawasdh3tlVydEGX9c3Ah6XcOLoTOYUZjFnXDZzCrOHTOu5yRegvslPXbOP441+e9zk\no67JT32zn2SPi8zUJDJTkshM9ZCZkkSa1z1oPpv+KsblwDN9cVNHKHMFkG+MmeqcfwP4NVBsjNkV\nku9rwEPAbGPMh52VqWIcQiBgg7jqKluFtmWfYscfB9M8yTYKO3jf4a1OK3cllKyG+vYrJ/UMgdEz\nbQu3aBEUzgdfY1tXdsX2Plc5qnhSYebn4bQbYdRJvS+nthzWPWKnKD0WHhEtMG0pLLwVCub0ydyO\nCAQMD63czX+8vL3dcpEjhnn55RdnR3Vxj/5C5bFG3t5VyaodFazaWRHRQ9BbctOTmVOYxawTsnq8\nkpfHJaR63Qzzekjzup3NY9OS3aQleUhLdpMUwWsRCBjqm/1WDJv8HA8Ko3Mcug+dnrVlOtaQ6VmD\n07UGjxt8IeU2+qhv9ne5hGkk3C4hM8XTRqQzku0+zeuhyR+godlPY7PdN/j8NDjH9aHpzX4afAHc\nLiEvPZmRmcmMzEhmZEaK3Wfa4zzneMSw5B4v19oV/VWM3wC8xphFYekrHFvO7EWZD2DHKXuc898C\nnzfG5ITlmwVsAK40xnTa1FMxjgG9EucI4pua1fktteV9E2dvBoyY0NbtnJIF1SVt3dpH9kHkHo+O\nKVoMp90EUy7sfiDb/vdh7W9h89MQiDxsqQ3jz7CiPPGc1hejKPJeSRXf/PN6yo+2jdQWgVvOmcQt\n506K+o9ZPDlYU8+6vdW8X1LFuyXVbP0kPA60c8YMT6G6rpn65shjv+NNkltITbJC3ewPtLRGlci4\nXUJuurdFrMeNGMadl0zrU5n9VYxvBR4AJhtjdjtpRdihTbcbYx7sYXku4G0g2xgzxUm7C/gxMCk0\natvpE/4dcIYxZmVn5aoYx4FI4txwBPKn90x8uyKSOLcIbpjo5kyEYbndEzFfk3UTB4O8QgO+uhLq\n4YUw7/qOXdhBV/Ta38GBXn4P82fCwn+DaZ8Bdw8GTQQCti7719lnV+60Q9QK5sLYuZA/g4oG+NZf\nN7ByR0W72xcWj+CXXziZvIzk3tlN25Zb0J3Z9thHQ3OArNQkxmSlMiYrlRHDvD1eLMMfMGwrO8r7\ne6tZV1LNupIqDh5p6FEZWWlJLJyYy8LiXBYV51I4Ig2fP8C2slrWl1azfm8160trOl1WU+m/TMgb\nxhu3ndWnMvqrGA8DPgTqsVHQBtuvmwHMNMYcc/KNA3YB9xhj7nHS7gJygNVAGbbP+DrgU8CXgq1d\nR9w3Onnuw/YZz8VOAPIxMK+jIK4gKsaDGL/Ptkpj2R9VVwXr/wDvPgxH93ecz5MKs74A826EUdPs\ni8P7j1p3dDtXdAjeDJj9JRuNvva3UP5Rx3mzxtk+5dlfjhxUdryiVXj3r4OD61tnXYuE2wv5MwgU\nnMKrR07gpxuHsceMInTE4MiMZBYW59IcFqFtjyNHbTf6/NQ1+nvdcvO6XYzOSmHM8FRGZ6VQkJXK\n2Aw347xHKHDXkGcqEQJsck1j9eFU1u2t4oPSGo41dq/Pt+U5HhenFmWzqDiPRcW5nDQms1svARXH\nGluEeX1pNRv319DQ3EPPygDF7RLSktykJdsWetCtnur1kJrkotEX4Gh9M0cbfM6+ud98NqdPyOHJ\nG+b3qYx+KcbQMlVl6HSYr2PdzCUheYqwk3ncbYy5y0lbCtwKTAeGY8X2Q+x0mKvDnjENuAuYjx0C\ntQ94DrjPGFPdlY0qxkpU8Ptg+zLbwt27uvO8+TPh0NbOXdE5E23f86wrISXTphkDO1+H1b+0HoaO\nSBthXeRFi+Dghlbxren7qIEa0tngn8gGM5ENgWK2BMYhGFKkiVSaSKGJFLH7ZJw05zx4DaDBeGnA\nbvXOcaNz3mC81IccN+MhR46SL9XkSxX5VNm9VLWk5Up7F7PfCC8HTuUx3xLeNVPpatoBEThpTCYL\ni3NZXJzH3KLsHvfvRqLZH2DbJ7b1XFpVR6CHv8MtLucmP8eb/NQ3OcFRzY4HodFPXbO/Xf9+kJQk\nF8OcPuZgf3Na2HFKkpvkJBfJIVOw2ulY7fSsXo/LTtHqceF1u1vOhyVboQ0KsNft6nEwVqPPT22L\nOLeK9NF6H3VNPpI9LpKTrI2pSW5SklykJLlJ8bQeJ4ekNfkDHDrawKHaRrsdbeBw8Li2gUNH7XH4\nDHZLZ43hV1ee3CPbw+m3YjwQUDFWok7ZJivKm/5uJ0rpCcXnWRGeeG7nC3Xsfx9W/wK2vkC/jTLv\nR2wNFPKofwnP+hfS6MwV5PW4mFkwnLlFOcwdl80p47LJHubtoqT+iTGGxpZIZj9JbrFBXknuAd2v\nH0samv0tIn24toER6cmcWtS30RAqxn1AxViJGccrrQv7vd937sIOuqLn3QC5xT17RsUOePtX8OGT\n7WdF6w7Jw6HgZNtPnD8DakqdlvT7nds8QDnmymDH2Mtxn3YdU6ZM6/myk3VVcGB92z72cQvtGPpo\nDmtTBiQqxn1AxViJOR25sCO5ontLbRms+Y0dk9zYQVSwy2OHXAUDtArmwojijlvgtWVw4P3WfuYD\nH8RhPHnvCCDUuHIoJ4d9viwKKWOqa1/HN4gbpl5k3fnjFkSOK/A1WS/HgfdbXf1Vu9rnswXCqOmt\nk9qoOA9JVIz7gIqxElfKNtlI77wpMP6s6K8Z3XDECvKWZ+zUnvkzW4V39ExI6sM80wG/bYkHhenA\nOjiyH9zJIWPSnc2TErJPaTtuHZw5zxtC9g0h85/Xtb+WmmVnbcsYDZljWrcMZ58+qiWSPBAwNPv9\nJO9/xwa9bX+x84j3UTPsS1Hh6W372Ms29s7bAKg494FAAI6UQvlmOHoQvOmQmm23tBy7T8nq2ciB\nOKFi3AdUjBVlkFO9F957GNb/ERpqEmSEI86Fp1lx6QkBX4QXlAgLuDTX2WNfg/WChE/U09kkPt40\nK3DhopeaDak5sVstrb4GDm2xwhvcDm2BpmNd35s83L6khdvrHUav1gjKKoRTr+s6XyeoGPcBFWNF\nGSI01cGmv9nugkNbeleGuGDkSTD2FCuulbusp6P8IwZ1IJ0nNUyos6wYRhL20H1SWquXxOWx4/OD\nglu+2Y7T7y8UzoevLu9TEd0V4/7XplcURYkX3jQ45VqYc40V0O64sDPHWuEtOMW6+sfMdlpeYdRV\nhSx8shLKBpk4++qhth5qDybakkGBirGiKIoIjF9st6ALO9jHPmp6ax/72LmQkd+9MtNybEDY1Ivs\n+WAX51iTMtz+LUZMtK74+mq71VXZfcMRBvLnqW7qCKibWlGUmFNXBXvftsOhejrPucvTNiiuXYBc\nMC2kPzjg66BvOXzv9EU3Hbf96aGC17JV2fJigcsDuZNh5DQb6T9qup2hLrOg85nzAn4ryOEiXV9t\n69QbMgvsLHl9QN3UiqIo/Zm0HDjx4jg+MDmyO703GGMDqtqIXhU0Hmsv8M31nUTKN0L6yLaimzvZ\nrgDXU1xu+5kO0Ah1FWNFURSlZ4hAcobdsgoTbc2gIMoDGhVFURRF6SkqxoqiKIqSYFSMFUVRFCXB\nqBgriqIoSoJRMVYURVGUBKNirCiKoigJRsVYURRFURKMirGiKIqiJBgVY0VRFEVJMDo3dQRE5DCw\nN8KlXKAizub0F7TuQ5ehXH+t+9AlWvUfZ4zJ6yqTinEPEJF13ZnwezCidR+adYehXX+t+9CsO8S/\n/uqmVhRFUZQEo2KsKIqiKAlGxbhnPJRoAxKI1n3oMpTrr3UfusS1/tpnrCiKoigJRlvGiqIoipJg\nVIwVRVEUJcGoGHeCiJwgIv8rIkdE5KiIPCUihYm2Kx6IyFkiYiJsNYm2LdqIyFgR+S8ReUdE6px6\nFkXIlyIi/yEin4hIvZP/jPhbHD16UPdI3wUjIrPjb3V0EJHLReQfIrLX+XtuF5H7RSQjLF+2iDws\nIhUiclxEXhORGYmyOxp0p+4iUtTJ3z0rkfb3FRFZIiJviEiZiDSKyH4R+ZuITAvLFzcN8MSi0MGA\niKQBbwCNwDWAAe4F3hSRmcaY44m0L47cArwXcu5LlCExpBj4PPA+sBI4v4N8vwcuAr4L7Aa+Abws\nIvONMRviYWgM6G7dAR4DfheW9nFszIoL3wFKgR8A+4GTgbuAs0VkgTEmICICPA8UAf8KVAPfx/4O\nzDbG7E+E4VGgy7qH5L0feC7s/tp4GBlDcrDf+d8Ah4FC4HZgjYjMMMbsjbsGGGN0i7AB/wb4geKQ\ntPFYMfp2ou2LQ/3Pcr58n0q0LXGoqyvk+Hqn3kVheWY56V8JSfMA24HnEl2HWNbduWaAexNtb5Tr\nnhch7Wqnruc455c652eH5BkOVAG/SnQdYlz3Iuf8+kTbG6fPZIpT39uc87hqgLqpO2YpsMYYszOY\nYIzZA6zG/oMqgwTTthXQEUuBZuCvIff5gCeBJSKSHCPzYko36z4oMcYcjpAc9AIVOPulwEFjzJsh\n9x3BtpYH7O9AN+s+1Kh09kHvX1w1QMW4Y04CPoqQvhmYFiF9sPKEiPhFpFJE/jxU+swjcBKwxxhT\nF5a+GfBi3b2DnZud/rU6p79tcaINigFnOvutzr6z34FCEUmPi1XxIbzuQe4XEZ/Tb/rcQO8vD0VE\n3CLiFZFJ2C6YMuAvzuW4aoD2GXdMDrZ/KJwqIDvOtiSCI8CDwFvAUWyf0g+Ad0TkZGPMoUQalwA6\n+z4Erw9mHgdeAA4C47D95m+IyHnGmBWJNCxaiEgBcA/wmjFmnZOcA5REyB78u2cDx2JvXWzpoO6N\nWIF6BduvOhX7G/C2iMwzxoSL9kBkLXCKc7wT66IP/rbFVQNUjJWIGGM+AD4ISXpLRP4JvIsN6vpR\nQgxTEoIx5qqQ05Ui8iy21XAvsCgxVkUPp4X7LNZF+ZUEmxNXOqq7MeYT4KaQrCtFZDm2ZfhD4F/i\naWeMuArIBCZgg9peFZFFxpiSeBuibuqOqSby209Hb0uDHmPMemz07KmJtiUBdPZ9gNaW0pDAGFML\nLGMQfBdEJBXbBzwBWGLaRkh39Xcf0L8FXdS9HcaYfcAqBsHfHcAYs9UYs9YY8xfgXCAdG1UNcdYA\nFeOO2YztMwhnGrAlzrb0N4biHKqbgfHOcIdQpgFNWBfXUGRAfxdEJAn4X2Au8GljzKawLJ39DpQa\nYwasi7obde+MAf13j4Qxpgb7fxyM/4irBqgYd8xzwOkiMiGY4EyGsJD2Y+6GBCIyFxv+/26ibUkA\nzwNJwBXBBBHxAF8AXjHGNCbKsEQgIpnAxQzg74KIuIAngHOAzxhj1kTI9hxQICJnhtyXCVzCAP4d\n6GbdI91XiO2WGLB/944QkVHYfvFdTlJcNUAXiugAERkGfAjUY/tHDfATIAOYOZDfiLuDiDwB7AHW\nAzXYAK7vA3XAHGNMRQLNizoicrlzeC62n+zr2KCVw8aYt5w8TwJLsMFLe4CbsYK0wHHhD0i6qruI\nfAf7EvYmrQFcwbRzjTEr42913xGR/4et733Y4LRQ9htj9juitQo4Aft3D076MROY5bhtBxzdrPuD\n2AbbO9jvwxRs3YcDpxljtsfR5KgiIk9jf9s2YgNUJwPfAvKBecaYj+OuAYkeaN2fN+ysLP9w/li1\nwDNEmBBhMG7Yf7qN2KjqZmAfdkmx0Ym2LUb1NR1sK0LypAI/xw5/aMBGYp6VaNtjXXdsK3A1UOF8\nFyqxLYN5iba9j/Uu6aTud4XkywEewcYF1AGvY4U44XWIZd2Br2LHHlc7f/cy4M/AlETbH4X6fw87\nA1eN8zfdjo0cLwrLFzcN0JaxoiiKoiQY7TNWFEVRlASjYqwoiqIoCUbFWFEURVESjIqxoiiKoiQY\nFWNFURRFSTAqxoqiKIqSYFSMFSUOiMhVIlIacr5FRL4e5WfMF5G1InJcRIyIzO4g310iYkLOs5y0\nOdG0pyeIyGzHhnarXzl1uSsBZilK3FAxVpT4cAp2koHgKjlTgudR5PfYldguAeZjF/WIxMPO9SBZ\nwI+BhIkxMNuxIdJSlPOxNivKoEWXUFSU+HAK8LJzPAcIYKfaiwrOtI1TgPuMMW90ltfYlXk6XZ0n\nCvYIkGSMaeprWaab8yYrykBGW8aKEmMcoZxNa0t4LrDFGNPQzfszReTXInJQRBpFZLuIfMsRPETk\nWsCP/X++w3HrlnRSXoub2pn4fo9z6X+ce41TZjD/ZSKyRkTqRKRGRP7uLBgQWmaJiDwuIl8VkW3Y\nlawucq7dLSLrReSoiFSIyBsicnrIvdcCjzqnO0JsKHKut3NTi8gFIvKOiNSLyBEReUZEpoTlNb2U\n6gAABJRJREFUWSEiq0TkU87z60TkIxH5bFi+ySLytIgcEpEGESl16qiNFSVuqBgrSoxwBMpghTId\neNE5fxCYGS46HZThwq4b/BXnvkuA5dg5su9zsi3DrqQD1lU9H/gs3eMT4DLn+H7n3vlOmYjITdi5\nebcAlwM3AtOBt0QkI6yss4FvA3cDF2DnNgcoAH4BXApcCxwC/ikiM0Lsv9c5viLEhk8iGSwiFzj3\nHMOumnWzY9MqESkIyz4R+E/s53WZU+bfRaQ4JM8yx8absQuB3A40or+PSjxJ9ITduuk2WDfsuqez\nsUKw2TmejZ10/lsh595OyrgYO3n/tWHpD2MFI9c59xC2wEEnZd5l//Vbzouce68Py5eOXSjkkbD0\n8diW760haSXYCffzu3i227F1O/CfIenXOjYUR7gnfOGGdcAOwBNmUzPw85C0FU7apJC0kdiXox84\n57lO+UsT/X3RbWhv+uanKDHCGLPFGLMBu/zeCuf4OHYJtr8bYzY4W2f9qmdg+5f/HJb+OOClbSBW\ntJkPZAJPiIgnuGFX8Nrm2BbKGmNMWXghjpv4TRGpBHxYgZyM7ePuEc6ydnOAvxpjfMF0Y8we7MpS\nZ4bdssMYsyMk3yFsyzzoZq8EdgM/FZGvicikntqkKNFAxVhRYoCIuEPEayHwjnO8GDgAlDnXpYui\ncoCqCIJdFnI9Vox09q9hBTR0mwGMCMvfzq3sDJd6EetSvg44HTgVG7yW0gubsgGJ9CzsZxL+eVRF\nyNcYfLYxxgDnYVvb9wMfi8huEbm5F7YpSq/RAAVFiQ2v07aV9idnC9Ls7M/GulM7ogrIERFvmCDn\nh1yPFZXO/lqsmz2c2rDzSOuxfg7bGr7MGBOsMyKSjV1LtqdUO8/Jj3Atn158HsaY3cDVzovRLOCb\nwG9EpMQY81IvbFSUHqMtY0WJDTdiW4APADud41OBw8CPQs67Gmv8Fvb/9Iqw9C9j+23fiYKtjc4+\nNSz9bazgFhtj1kXYtnej7DRsH23oJCPn0Oom7sqGNhhjjmM/sytExB1S5jhgAZ2/2HSKsWzABqGB\nDQpTlLigLWNFiQFBoRKRO4Blxph1ztCbXOD3kfpWO+AlYBXwWxHJw7ZQPw1cD9xvjKmIgrnl2Fbw\nF0VkI7Zfe48xplJEvgv8t/Psl7ABXQXYVv8KY0x4X3Y4y4FbgcdE5FFsX/EdWFd9KFuc/TdE5A9Y\nz8HGDvrT78BGQL8gIr/BBprd7dj2YA/qjYjMxEZb/xX70uTGegJ8QKfjtRUlmmjLWFFihIh4gXOx\nggRwIfBBD4QYY0wAO173D8D3sCJ0Ebb19sNo2Ok843psf+xrwHvYIVQYY34HLMUGW/0J2/97F/ZF\nfkM3yn4ZuAXbb/4C8FXgaqzwheb70Cn3EuzLx3vAmA7KXI79DLKAvwG/BbYCi4wxB7tZ7SBlQCn2\n83wO+Ivz3IuNMdGeIU1ROkRs/IKiKIqiKIlCW8aKoiiKkmBUjBVFURQlwagYK4qiKEqCUTFWFEVR\nlASjYqwoiqIoCUbFWFEURVESjIqxoiiKoiQYFWNFURRFSTD/H40sWdL7IrlKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f0ad748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz Question:** From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_py2]",
   "language": "python",
   "name": "conda-env-ipykernel_py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
